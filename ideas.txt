0) representation of a tensor contraction (propagational point of view) -> done
0b) tensor theory of graphs -> ??

On convolutions on graphs:
1) tensor-train or other tensor decompositions to accelerat SWX -> useless approach
2) graph product (approximate) factorization (to make nd convolutions) -> only if wrt symmetries
3) decomposition of general graphs into circulant graphs -> flawed (circ = node ordering)
4) shifts (done already)

-1) translation on the covariance graph : preserve classes (class invariant) -> just a point

Todo
X Emphasize on lossless superiority before -> done
X Add figures -> wip
X Rewrite remark on expressivity to better explain the refs -> done
X Reformat the ordering of the subsections in chapter 1, clean toc on 3 levels -> done
x Pass on bib files, and add venues instead of arxiv when relevant

notes for IV:
x few notes on DTW loss vs conv1D

research projects
x dynamic node warping: adapt dynamic time warping to graphs
x multi view CNN after node embedding
x distill paper ?
  -> ternary representation = new point of view
  -> influence of symmetries on distorded domains
  -> improving local receptive fields of CNNs
