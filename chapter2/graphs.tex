\section{Graphs}

\subsection{Basic definitions}

\begin{definition}\textbf{Graph}\\
A \emph{graph} $G$ is defined as a couple of finite sets $\langle V,E \rangle$ where $V$ is the set of \emph{vertices}, also called \emph{nodes}, and $E \subseteq\binom{V}{2}$ is the set of \emph{edges}. It is associated with a weight mapping $w: E \rightarrow \bbr^*$. The number of vertices $|V|$ is called its \emph{order}.
\end{definition}

\figref{fig:graph} illustrates an example of a graph. Note that we employ interchangeably the terms \emph{vertex} and \emph{node}.

\begin{figure}[H]
\centering
\begin{tikzpicture}
\draw (0,0) -- (4,0) -- (4,4) -- (0,4) -- (0,0);
\node at (2,2){placeholder};
\end{tikzpicture}
\caption{Example of a graph}
\label{fig:graph}
\end{figure}

\begin{definition}\textbf{Adjacency matrix}\\
The \emph{adjacency matrix} of a graph $G = \langle V,E \rangle$ of order $n$, is a $n \times n$ real-valued matrix $A$ associated to an indexing of $V = \{v_1, v_2, \ldots v_n\}$, such that
\begin{gather*}
A[i,j] =
 \begin{cases}
   w\left(\{v_i,v_j\}\right) & \text{if } \{v_i,v_j\} \in E \\
   0 & \text{otherwise}
 \end{cases}
\end{gather*}
 \end{definition}

\begin{definition}\textbf{Degree and degree matrix}\\
The \emph{degree} of a vertex $v \in V$ is defined as $\deg(v) = |\{u \in V, \{u,v\} \in E\}|$.

The \emph{degree matrix} of a graph $G = \langle V,E \rangle$ of order $n$, is the diagonal matrix $D$, associated to an indexing of $V = \{v_1, v_2, \ldots v_n\}$, such that $D = \diag(\deg(v_1), \deg(v_2), \ldots, \deg(v_n))$.
\end{definition}

\begin{definition}\textbf{Laplacian matrix}\\
The \emph{laplacian matrix} of a graph $G = \langle V,E \rangle$ of order $n$, associated to an indexing of $V = \{v_1, v_2, \ldots v_n\}$, is defined as $L = D - A$, where $D$ is the degree matrix and $A$ is the adjacency matrix.
\end{definition}

\begin{definition}\textbf{Digraph}\\
A digraph is an oriented graph \ie $E \subseteq V \times V - \{(v,v), v \in V\}$, and its weight mapping $w$ and adjacency matrix $A$ are not symmetric. Notions defined on graphs naturally extends to digraphs where possible.
\end{definition}

\begin{definition}\textbf{Bipartite graph}\\
A \emph{bipartite graph} is a triplet of finite sets $\langle V^{(1)}, V^{(2)}, E \rangle$, where $V^{(1)}$ and $V^{(2)}$ are sets of vertices, $V^{(1)} \cap V^{(2)} \neq \emptyset$, and $E \subseteq V^{(1)} \times V^{(2)}$. It is associated with a weight mapping $w: E \rightarrow \bbr^*$. Its adjacency matrix $A$ is associated to indexings of $V^{(1)} = \{v^{(1)}_1, v^{(1)}_2, \ldots v^{(1)}_n\}$ and $V^{(2)} = \{v^{(2)}_1, v^{(2)}_2, \ldots v^{(2)}_n\}$, such that
\begin{gather*}
A[i,j] =
 \begin{cases}
   w\left((v^{(1)}_i,v^{(2)}_j)\right) & \text{if } (v^{(1)}_i,v^{(2)}_j) \in E \\
   0 & \text{otherwise}
 \end{cases}
\end{gather*}
\end{definition}

\subsection{Special classes of graphs}

Our goal is to define lattice graphs using group theory.

\begin{definition}\textbf{Infinite graph}\\
An \emph{infinite graph} is defined by natural extension of the notion of graph $G=\langle V,E \rangle$ where $V$ and $E$ can be infinite. We denote $\order{G} = \infty$.
\end{definition}

\begin{definition}\textbf{Graph automorphism}\\
A graph automorphism of a graph $G$ is a bijection in the vertex domain $\phi: V \arrow V$ such that $\{u,v\} \in E \Leftrightarrow \{\phi(u), \phi(v)\} \in E$
\end{definition}

\begin{definition}\textbf{Lattice graph}\\
A lattice graph is a graph, possibly infinite, that admits an automorphism, or is an induced finite subgraph of such.
\end{definition}

%\subsubsection{Grids}

%\subsubsection{Lattices}

%\subsubsection{Spatial graphs}

%\subsubsection{Projections of spatial graphs}


\subsection{Graphs in deep learning}

We come across the notion of graphs several times in deep learning:
\begin{itemize}
\item Connections between two layers of a deep learning model can be represented as a bipartite graph, the \emph{connectivity graph}. It encodes how the information is propagated through a layer to another. See \secref{con_graph}.
\item Neural architectures can be represented by a graph. In particular, a computation graph is used by deep learning programming languages to keep track of the dependencies between layers of a deep learning model, in order to compute forward and back-propagation. See \secref{comp_graph}.
\item A graph can represent the underlying structure of an object (often a vector or a signal). The nodes represent its features, and the edges represent some structural property. See \secref{inductive_graph}.
\item Datasets can also be graph-structured. The nodes represent the objects of the dataset, and its edge represent some sort of relation between them. See \secref{transductive_graph}.
\end{itemize}

%\subsection{Graphs related to models}

\subsubsection{Connectivity graph}
\label{con_graph}

A Connectivity graph is the bipartite graph whose adjacency matrix is the connectivity matrix of a layer of neurons.
%$U = \{u_1, u_2, \ldots, u_n\}$
Formally, given a linear part of a layer, let $\textbf{x}$ and $\textbf{y}$ be the input and output signals, $n$ the size of the set of input neurons $N = \{u_1, u_2, \ldots, u_n\}$, and $m$ the size of the set of output neurons $M = \{v_1, v_2, \ldots, v_m\}$. This layer implements the equation $y = \Theta x$ where $\Theta$ is a $n \times m$ matrix.

\begin{definition}
{The \emph{connectivity graph} $G = (V,E)$ is defined such that $V = N \cup M$ and $E = \{(u_i,v_j) \in  N \times M, \Theta_{ij} \neq 0 \} $.}
\end{definition}

I.e. the connectivity graph is obtained by drawing an edge between neurons for which $\Theta_{ij} \neq 0$.
For instance, in the special case of a complete bipartite graph, we would obtain a dense layer. 
Connectivity graphs are especially useful to represent partially connected layers, for which most of the $\Theta_{ij}$ are $0$. 
For example, in the case of layers characterized by a small local receptive field, the connectivity graph would be sparse, and output neurons would be connected to a set of input neurons that corresponds to features that are close together in the input space. \figref{con_ex} depicts some examples.

\begin{figure}[h]
  \begin{center}
    \begin{tikzpicture}
      \tikzstyle{every node} = [draw, circle, thick, inner sep = 2pt]
      \foreach \y in {0,...,4}{
        \pgfmathtruncatemacro{\yplusone}{5 - \y}
        \node(a\y) at (0,.6*\y) {\footnotesize\yplusone};
      }
      \foreach \y in {0,...,4}{
        \pgfmathtruncatemacro{\yplusone}{5 - \y}
        \node(\y) at (2,.6*\y) {\footnotesize\yplusone};
      }

      \foreach \x in {0,...,4}{
        \foreach \y in {0,...,4}{
          \path[opacity=0.5] (a\x) edge (\y);
        }
      }
    \end{tikzpicture}
  \end{center}
  \caption{Examples}
  \label{con_ex}
\end{figure}

\todo{\figref{con_ex}. It's just a placeholder right now}


Connectivity graphs also allow to graphically modelize how weights are tied in a neural layer. Let's suppose the $\Theta_ij$ are taking their values only into the finite set $K = \{w_1, w_2, \ldots, w_\kappa\}$ of size $\kappa$, which we will refer to as the \emph{kernel} of \emph{weights}. Then we can define a labelling of the edges $s: E \rightarrow K$. $s$ is called the \emph{weight sharing scheme} of the layer. This layer can then be formulated as $\displaystyle \forall v \in M, y_v = \sum_{u \in N, (u,v) \in E} w_{s(u,v)} x_u$. \figref{cnn} depicts the connectivity graph of a 1-d convolution layer and its weight sharing scheme.

\begin{figure}[h]
  \begin{center}
    \begin{tikzpicture}
      \tikzstyle{every node} = [draw, circle, thick, inner sep = 2pt]
      \foreach \y in {0,...,4}{
        \pgfmathtruncatemacro{\yplusone}{5 - \y}
        \node(a\y) at (0,.6*\y) {\footnotesize\yplusone};
      }
      \foreach \y in {0,...,4}{
        \pgfmathtruncatemacro{\yplusone}{5 - \y}
        \node(\y) at (2,.6*\y) {\footnotesize\yplusone};
      }
      \path[opacity=0.5]
      (a0) edge (0);
      \path[dashed]
      (a0) edge (1);
      \path[dotted]
      (a1) edge (0);
      \path[opacity=0.5]
      (a1) edge (1);
      \path[dashed]
      (a1) edge (2);
      \path[dotted]
      (a2) edge (1);
      \path[opacity=0.5]
      (a2) edge (2);
      \path[dashed]
      (a2) edge (3);
      \path[dotted]
      (a3) edge (2);
      \path[opacity=0.5]
      (a3) edge (3);
      \path[dashed]
      (a3) edge (4);
      \path[dotted]
      (a4) edge (3);
      \path[opacity=0.5]
      (a4) edge (4);
    \end{tikzpicture}
  \end{center}
  \caption{Depiction of a 1D-convolutional layer and its weight sharing scheme.}
  \label{cnn}
\end{figure}


\todo{Add weight sharing scheme in \figref{cnn}}

\subsubsection{Computation graph}
\label{comp_graph}

%\subsection{Graphs related to data}

\subsubsection{Underlying graph structure and signals}
\label{inductive_graph}

\subsubsection{Graph-structured dataset}
\label{transductive_graph}

%transductive vs inductive
