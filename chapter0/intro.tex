\chapter*{Introduction}\label{chp:int}
\addcontentsline{toc}{chapter}{\nameref{chp:int}}

One of the first appearance of the \emph{convolution} was in the eighteenth century in D'Alembert's work \citep{wiki:enc}. Since then it has been used in a wide range of domains, including applied mathematics, physics, engineering, informatics and real world problems. In today's era of computerized industry and big data, the convolution has never been more useful. A famous example is its usage in deep learning algorithms for image processing \citep{lecun2015deep}. But this is just the tip of the iceberg. Years after years, IT companies aquire more and more data. With hashing algorithms, accessing single points or moderately sized batch of data is relatively easy. However, processing the entire database's knowledge is another story. Algorithms that don't scale well are too time consuming, so only those that traverse the entire database only a few times remain feasible. And that's the point: traversing the database (done in a parallelized manner) to process it can be modelized with a convolution. Therefore this little mathematical tool is bound to play a great role in our current and future civilizations ! % Now if we cut the bullshit and comeback down to earth, one 
A simple example can be seen in the company that funds this phd. One of its technology for processing large quantities of data is a programming language centered around a few frameworks. One of them %, that is called MAP\footnote{\url{http://www.warp10.io/reference/frameworks/framework-map/}} (in reference to another algorithm called MAP/REDUCE),
is nothing else than a reimplemention of the convolution by various complicated functions, which helps tremendously to produce simplified scripts.

%In this thesis work, our study is restricted to convolutions of graph signals and to deep learning models that can make use of it. This is a timely subject,

The subject of this thesis is a timely one, since deep learning have never received so much spotlight than in the last decade. However, deep learning models are often very specialized to their use cases. Standard Convolutional Neural Networks (CNNs) can only be applied to datasets for which each object can be modelized by a signal defined on an euclidean domain, like images or sounds. This is because the discrete convolution takes into account the structure of the domain of its inputs, in addition of their raw data. Our goal is to study and find ways to extend deep learning models to signals defined on a broader range of domains. To this end we build an algebraic theory of convolutions of graph signals, with the hope to characterize what a more generic definition of convolution should be and what properties should it preserve to keep its usefulness in deep learning models. We choose to modelize the domain of signals we study with a graph, since this structure can represent a wide range of domains. Our subject fits in the idea that efforts in the Artificial Intelligence (AI) field should tend toward a general-purpose AI, and in a lesser extent, that AI based algorithm, like deep learning, should seek genericity.

\begin{displayquote}
\begin{flushright}
\emph{The ultimate aim is to use these general-purpose technologies and apply them to all sorts of important real world problems.\\
--- Demis Hassabis}
\end{flushright}
\end{displayquote}

This manuscript is broke down in three chapters. Each chapter is preceded by a short overview so that the reader can grasp its essential developments at a glance. In \chapref{chap:1}, we present our domains of interest with a selected literature review. Then, in \chapref{chap:2}, we theorize an algebraic understanding of convolutions of graph signals. Finally, in \chapref{chap:3}, we study neural networks intended for graph domains.


