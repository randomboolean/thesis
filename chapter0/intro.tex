\chapter*{Introduction}\label{chp:int}
\addcontentsline{toc}{chapter}{\nameref{chp:int}}

One of the first appearances of the \emph{convolution} operation was in the eighteenth century (\cite{d1754traite}, according to \cite{dominguezorigin}). Since then it has been used in a wide range of domains, including applied mathematics, physics, engineering, computer science and real world problems. In today's era of computerized industry and big data, the convolution has never been more useful. A famous example is its usage in deep learning algorithms for image processing \citep{lecun2015deep}. But this is just the tip of the iceberg. Years after years, IT companies acquire more and more data. With hashing algorithms, accessing single points or moderately sized batches of data is relatively easy. However, processing the entire database's knowledge is another story. Algorithms that do not scale well are too time consuming, so only those that traverse the entire database only a few times remain feasible. And that is the point: traversing the database (done in a paralleled manner) to process it can be modeled with a convolution. Therefore this little mathematical tool is bound to play a great role! % in our current and future civilizations! % Now if we cut the bullshit and comeback down to earth, one 
An example can be seen in the company that funded this Ph.D.: one of its technologies for processing large quantities of data is a programming language centered around a few frameworks. One of them %, that is called MAP\footnote{\url{http://www.warp10.io/reference/frameworks/framework-map/}} (in reference to another algorithm called MAP/REDUCE),
is nothing more than a reimplementation of the convolution by various complicated functions, which helps tremendously to produce simplified scripts.

%In this thesis work, our study is restricted to convolutions of graph signals and to deep learning models that can make use of it. This is a timely subject,

The subject of this thesis is a timely one, since deep learning have never received as much spotlight as in the last decade. However, deep learning models are often very specialized to their use cases. Standard Convolutional Neural Networks (CNNs) can only be applied to datasets for which each object can be modeled by a signal defined on an Euclidean domain, like images or sounds. This is because the discrete convolution takes into account the structure of the domain of its inputs, in addition to raw data. Our goal is to study and find ways to extend deep learning models to signals defined on a broader range of domains. To this end we build an algebraic theory of convolutions of graph signals, with the hope to characterize what a more generic definition of convolution should be and what properties it should preserve to keep its usefulness in deep learning models. We choose to model the domain of signals under study with a graph, since this structure can represent a wide range of domains. Our subject fits the idea that efforts in the Artificial Intelligence (AI) field should tend toward a general-purpose AI, and in a lesser extent, that AI-based algorithm, like deep learning, should seek genericity.

\begin{displayquote}
\begin{flushright}
\emph{The ultimate aim is to use these general-purpose technologies and apply them to all sorts of important real world problems.\\
--- Demis Hassabis}
\end{flushright}
\end{displayquote}

This manuscript is broken down into three chapters. Each chapter is preceded by a short overview so that the reader can grasp its essential contents at a glance. In \chapref{chap:1}, we present our domains of interest with a selected literature review. Then, in \chapref{chap:2}, we theorize an algebraic understanding of convolutions of graph signals. Finally, in \chapref{chap:3}, we study neural networks intended for graph domains.


