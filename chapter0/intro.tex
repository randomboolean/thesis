

One of the first appearance of the \emph{convolution} was in the eighteenth century in D'Alembert's work \citep{wiki:enc}. Since then it has been used in a wide range of domains, including applied mathematics, physics, engineering, informatics and real world problems. In today's era of computerized industry and big data, the convolution has never been more useful. A famous example is its usage in deep learning algorithms for image processing \citep{lecun2015deep}. But this is just the tip of the iceberg. Years after years, IT companies aquire more and more data. With hashing algorithms, accessing single points or moderately sized batch of data is relatively easy. However, processing the entire database's knowledge is another story. Algorithms that don't scale well are too time consuming, so only those that traverse the entire database only a few time remain feasible. And that's the point: traversing the database (done in a parallelized manner) to process it amounts to apply a convolution. Therefore this little mathematical tool is bound to play a great role in our current and future civilizations ! Now if we cut the bullshit and comeback down to earth, one simple example can be seen in the company that funds this phd. One of its technology for processing large quantities of data is a programming language centered around a few frameworks. One of them, that is called MAP\footnote{\url{http://www.warp10.io/reference/frameworks/framework-map/}} (in reference to another algorithm called MAP/REDUCE), is noting more than a re-implemention of the convolution by various complicated functions, which helps tremendously to produce simplified scripts. In this thesis work, our ambitions are more modest since our study is restricted to convolutions of graph signals. Our goal is to build a theory arounds them in view of understanding how deep learning algorithms can be extended to other domains than those they were originally intended for.

\todo{to be continued}
