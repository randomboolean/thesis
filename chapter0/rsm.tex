Ce manuscrit est une thèse soumise pour candidater au grade de docteur. Il est dévolu à deux sujets. Le premier traite d'extensions de la convolution discrète aux signaux sur graphe. Le second traite d'extensions de l'ensemble de définition des réseaux de neurones à des graphes. Les deux sujets sont reliés car les réseaux de neurones peuvent tirer profit de la structure sous-jacente de leur ensemble de définition à l'aide de convolutions.

%\paragraph{\h{0}}
Dans un premier temps, nous présentons les notions relatives à nos sujets d'étude. L'un deux, l'\emph{apprentissage profond}, est le domaine de recherche qui se concentre sur une classe particulière de fonctions: les \emph{réseaux de neurones}. Puisque nous essayons de suivre une approche rigoureuse, nous commençons par définir proprement leurs ensembles de définition et d'arrivée, qui peuvent être modélisés par des \emph{espaces tensoriels}. En particulier, nous donnons des définitions originales pour les tenseurs, qui sont appropriés dans le cadre de l'étude de réseaux de neurones. Nous expliquons aussi comment les données sont traitées et manipulées. Nous définissons certaines opérations binaires qui sont importantes pour notre étude : \emph{contraction tensorielle} et \emph{convolution}. Ensuite, nous définissons les réseaux de neurones, discutons leur interprétation biologique, montrons comment ils apprennent, et relatons quelques avancées historiques. Puis, nous introduisons des couches classiques, en particulier les couches convolutionelles pour lesquelles nous démontrons un premier résultat qui avance notre réflexion. Puis, nous présentons le domaine de recherche relatif à l'\emph{apprentissage profond sur graphes}. Nous commençons par des définitions relatives aux graphes, puis nous décrivons des cas d'usage. Enfin, nous faisons une revue des modèles à l'état de l'art en deux parties, l'une sur les méthodes spectrales, l'autre sur les méthodes dans le domaine des sommets du graphe.

%\paragraph{\h{0}}
Au cours du deuxième chapitre, nous formalisons mathématiquement les principes de l'extension de la convolution aux signaux sur graphes. Si le graphe n'est pas un graphe grille, il n'existe pas de manière naturelle d'étendre la convolution Euclidienne. Nous commençons par analyser les raisons pour lesquelles la convolution Euclidienne est utile en apprentissage profond. En particulier, nous rapellons une charactérisation classique : la classe des opérateurs de convolutions est exactement la même que celle des fonctions linéaires qui sont équivariantes aux translations. Nous nous demandons donc sur quels domaines une convolution avec ces propriétés peut être obtenue de manière naturelle. Cela nous amène à considérer la théorie des réprésentations et les convolutions définies sur des groupes. Puisque la convolution Euclidienne est juste un cas particulier de la convolution de groupes, cela est parfaitement pertinent de diriger notre construction dans cette direction. Ensuite, nous cherchons à transférer la définition de la convolution de groupes sur les sommets, par le biais de son groupe symétrique. Pour obtenir la charactérisation désirée, nous constatons que nous avons besoin de baser la convolution sur les actions du groupe, plutôt que sur ses éléments. Nous parvenons à l'obtenir, à conditier de fixer une carte équivariante entre le groupe actif et les sommets. Puis, nous proposons une expression mixte de cette convolution, définie entre un signal sur le groupe actif et un signal sur les sommets, pour laquelle nous démontrons que la charactérisation reste valable sous condition de commutativité. Puis, nous introduisons le rôle de l'ensemble des arrêtes du graphe pour voir comment il devrait influencer la construction. En particulier, nous définissons la notion de contrainte par les arrêtes et la notion de préservation de la localisation. Pour chacune, nous obtenons une charactérisation de graphes admettant une construction naturelle de convolutions avec cette propriété. Nous analysons les notions de localités et de partage de poids, et proposons une formulation dans le cas de petits noyaux de convolution. Grâce aux théorêmes obtenus, nous sommes capable de décrire les convolutions sur n'importe quel graphe, en tant que convolutions sur des sous-graphes appropriés. Ensuite, nous assouplissons certains contraintes de cette construction pour mieux l'adapter à des graphes quelconques. Nous expliquons pourquoi une construction basées sur des groupes est peu intéressante pour certains graphes, et nous introduisons la notion de groupoïde. Nous étendons la construction précédente avec des groupoïdes de tranformations partielles, et démontrons que la charactérisation par équivariance est préservée. Enfin, nous étendons encore notre construction avec un autre type de groupoïdes que appelons groupoïdes de chemins. Les groupoïdes de chemins permettent de traiter le cas le plus général, et pour eux nous obtenons aussi la charactérisation à condition de fixer un manière de traverser les sommets, mais au prix d'inclure des cas dégénérés.

%\paragraph{\h{0}}
Dans le chapitre final, nous cherchons à comprendre comment les réseaux de neurones peuvent être étendus à des ensembles de définition pour lesquels ils n'étaient pas conçus pour y être appliquable. Dans ce but, nous proposons une interprétation des opérations linéaires sous-jacentes afin d'améliorer notre intuition. Dans un premier temps, nous démontrons l'évident en expliquant avec plus de détails comment un espace tensoriel peut être interprété en temps qu'espace neuronal, tout en jonglant entre les représentations tensorielles et les représentations à bases de signaux. Puis, nous proposons une représentation basée sur des graphes. Entre deux couches, un graphe de propagation décrit cette propagation. Sur la couche en entrée, les neurones peuvent avoir une structure sous-jacente de graphe. Nous montrons une relation entre ces graphes, que nous obtenons si, et seulement si, les champs de réceptions locaux des neurones sont entremêlés. En introduisant la notion de partage de poids dans notre analyse, nous découvrons qu'une couche, quel que soit son ensemble de définition, peut être formulée par une opération ternaire et linéaire, que nous appelons \emph{contraction neurale}. Ses opérandes sont le \emph{signal d'entrée}~$X$, le \emph{noyaux de poids}~$\Theta$, et le \emph{schéma de partage de poids}~$S$. Nous écrivons $\wideparen{\Theta S X}$. Nous étudions ses propriétés, et sa généricité en comparaison avec les méthodes de la littérature. Nous proposons une méthode pour l'implémenter de manière efficace. Grâce à une expérience qui se base sur cette opération, nous observons en quoi l'exploitation de symétries est bénéfique, ce qui justifie l'utilisation de convolutions. A l'aide d'autres expériences, nous explorons des idées basées sur des tirages aléatoires pour appliquer cette représentation ternaire dans le cas de graphes quelconques. Puis, nous étudions la possibilité d'apprendre comment les poids sont partagés, ce qui revient à apprendre à la fois $S$ et~$\Theta$. Nous explorons cette piste pour des domaines de graphes, avec des expériences sur des grilles, sur des graphes de covariance et sur des réseaux de citations. Enfin, nous investiguons un exemple d'architecture convolutionelle appliquées pour des signaux sur graphe. La convolution est contruite à partir de translations sur graphe qui définissent le schéma de partage de poids $S$ de la couche convolutionelle. Nous présentons des modèles de translations et les approximations, les couches de sous-échantillonage, ainsi que la technique d'augmentation de données que nous utilisons. Au travers d'expériences, nous appliquons ce modèle à des graphes grilles et autres graphes y ressemblant.

%\paragraph{\h{0}}
Dans la conclusion, nous rappelons les différents résultats, avancées, et nouveaux modèles de réseaux de neurones que nous avons présentés tout au long de ce mémoire.