In this manuscript, after presenting the domains related to our subject, we built a theory for convolutions on graphs, in view of using them in CNN on graph domains. On enclidean domains, convolutional layers take advantage of the translational equivariances of the convolution. Therefore, our construction on graph domains depends on a set of transformations of the vertex set for which the resulting operation is also equivariant. More precisely, we wanted to define a class of convolutional operators that are exactly the class of linear operators that are equivariant to this set of transformation. We demonstrated that this characterization holds should this set have an algebraic structure of group, groupoid or path groupoid. In particular, we proved that this amounts to search for Cayley subgraphs. We also saw that the possible abelianity of these structures is linked with the property that the convolution is supported locally. Then, we studied neural networks intended for graph domains. We adopted an approach based on graph representations of the propagation between layers of neurons. We proved that if the local receptive fields of the neurons are intertwined, then if their input have a graph structure, it can be used to define the propagation. We also discovered that the linear part of a layer can be expressed by an operator that involves three operands: the input signal, the weight kernel and the weight sharing scheme. We called it \emph{neural contraction}, in reference to the term \emph{tensor contraction}. We showed that it is associative, commutative, and generic in the sense that it can represent any kind of layer. We used this representation to see the influence of symmetries that are present in the structure of the data. We conducted experiments to learn how the weights are shared in addtion of learning the weights. In a sense, this amounts, in the case of convolutions, to try to learn its set of transformations to which it is equivariant. We saw that it attains similar performances than other state-of-the-art models. Then we made experiments for a CNN for which the convolution is based on a set of translations on graphs, which define how the weights are shared. We proposed an alogrithm to find the translations. We defined downscaling and data augmentation from these translations, and used a residual network architecture. We showed that this model retrieves the performances of CNNs without feeding to it the underlying structure of images, and that it attains strong performances on graph signal datasets.

In conclusion, we proposed a novel layer representation for extending CNN architectures to other input domains than those for what they where intended. This, as pointed out in the introduction, participates in rending them more generic, and thus applicable to a broader range of real world problems. In the process, we also advanced our understanding of convolutions. We hope that the reader had pleasure reading this manuscript and that it gave him ideas and shed new lights. Let us thrive for a continuous effort to help advance collectively the boundaries of human knowledge at our scales and beyond ...