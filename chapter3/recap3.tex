\section{Conclusion}
\label{sec:3.5}

In this chapter, we developped a new perspective to look at neural networks. This led us to discover that neural layers can be formulated with a ternary operation, that we called \emph{neural contraction}, between an input signal $X$, a weight kernel $\Theta$, and a weight sharing sheme $S$. We studied this representation and proposed efficient implementations. We saw it can represent any kind of layer. In particular we showed how related works from the literature can be represented with neural contractions. Also, we used it see the influence of symmetries, and concluded on their critical role in the success of convolutions. Then, we tested models on the task of graph signals classification and on the task of semi-supervised classification of nodes. To construct the scheme $S$, we tested ideas based on randomizations, from which we derived what we called \emph{Monte-Carlo Neural Networks} (MCNN), and a technique we called \emph{graph dropout}. We also tested to learn the scheme $S$, obtaining \emph{Graph Contraction Networks} (GCT). Finally, we tested to infer the scheme $S$ from a set of translations inferred from the domain, which we called \emph{Translation-Convolution Neural Networks} (TCNN). On image and fmri datasets, GCTs and TCNNs obtained performances that match those of CNNs. On scrambled image datasets, TCNNs obtained almost the same performance as in the non-scrambled case, outperforming alternatives by a large margin. On text documents, MCNNs beat the other graph convolution alternative. On citation networks, GCTs set new state-of-the-art results, but by small margins that are not statistically significant.