\section{Related works}

\todo{Presentation and note on older graph neural network models}


%\subsection{Methodology}

\subsection{Analysis of spectral techniques}

Given a graph $\gve$, spectral techniques are based on the graph Fourier transform (GFT)~\citep{chung1996spectral,shuman2013emerging}, derived from the laplacian matrix of $G$. Spectral CNNs make use of a convolution defined as pointwise multiplication in the spectral domain defined by the GFT. Neural networks based on such methods were first introduced by \cite{bruna2013spectral}. \cite{henaff2015deep} later extended them to large scale classification tasks, and investigate the problem of estimating a suitable $G$ from data.

Let a graph $\gve$ of order $n$, with adjacency matrix $A$, degree matrix $D$, and let $L$ denote either its normalized laplacian matrix or its unnormalized laplacian matrix. As a real symmetric matrix, $L$ is diagonalized as $L = U^{\T} \Lambda U$, where columns of $U$ form the eigenvectors basis $\cu$ and $\Lambda = \diag(\lambda_1, \ldots , \lambda_n)$ is a diagonal matrix of the corresponding eigenvalues. 

\begin{definition}\textbf{Graph Fourier transform}\\
The \emph{GFT}, \wrt laplacian eigenbasis $\cu$, of a signal defined on~$G$, represented by a column vector $x \in \cs(G)$, is defined as $\cf(x) = Ux$.
\end{definition}

The spectral convolution can then be constructed by similarity with the convolution theorem from classical Fourier analysis~\citep{wiki:conv}.

\begin{definition}\textbf{Spectral convolution}\\
The spectral convolution between two signals $x_1, x_2 \in \cs(G)$ is defined as
\begin{align*}
x_1 \star x_2 & = U^{\T}(Ux_1 \circ Ux_2)\\
 & = U^{\T} \diag(Ux_1) \h{2} Ux_2
\end{align*}
where here $\circ$ denotes Hadamard product and $\forall v \in \bbr^n, \diag(v) = \diag(v[1], \ldots, v[n])$.
\end{definition}

This gives rize to a class of convolution layers. In the following definition we reinterpret them with the formalism introduced in \secref{sec:nn}.

\begin{definition}\textbf{Spectral convolution layer with $\co(n)$ weights}\\
A spectral convolution layer $(g,h)$ is such that its connectivity matrix is of the form:
\begin{gather*}
W_g = U^{\T} \diag(\theta) \h{2} U
\end{gather*}
where $\theta$ contains $n$ learnable weights.
\end{definition}

\begin{remark}Spectral convolution layer are also extended with feature maps and input channels. In fact $W_g := W_g^{p,q}$ where $W_g^{p,q}$ is the $(p,q)$ block corresponding to input channel $p$, and feature map $q$, of a larger connectivity matrix. But we omit the superscript for the sake of simplicity.
\end{remark}

Such layers have two main drawbacks:
\begin{enumerate}
  \item they produce $\co(n)$ weights instead of $\co(1)$ as in the case of classical two-dimensional convolutions,
  \item connections of $W_g$ do not depend on a locality notion based on the original adjacency matrix $A$.
\end{enumerate}

Hence, \cite{bruna2013spectral} suggest to alleviate these issues with the following construction (reinterpreted with our formalism):

\begin{definition}\textbf{Spectral convolution layer with $\co(1)$ weights}\\
A spectral convolution layer $(g,h)$ is such that its connectivity matrix is of the form:
\begin{gather*}
W_g = U^{\T} \diag(\omega) \h{2} U
\end{gather*}
where $\omega$ is obtained from smooth interpolation between a weight vector $\theta$ of size $r = \co(1)$ and a smoothing kernel $K \in \bbr^{n \times r}$ ie $\omega = K \theta$.
\end{definition}

In particular, \cite{bruna2013spectral}, argue that the second issue is answered by the fact that \quotes{in order to learn a layer in which features will be not only shared across locations but also well localized in the original domain, one can learn spectral multipliers which are smooth}. An argument that was also taken up by \cite{henaff2015deep}. This argument is suggested by similarity with the classical Fourier transform for which spatial decay relates with smoothness in the spectral domain.

\begin{claim} Smooth multipliers in the graph spectral domain produce a spatially localized operator in the vertex domain.
\label{cla:smooth}
\end{claim}

Although \claref{cla:smooth} is true for a grid of infinite size (corresponding to the case of the discrete Fourier transform (DFT)), it is not necessarily true for a general graph in finite settings. More precisely, as mentioned by \cite{henaff2015deep}, this argument in classical Fourier analysis is grounded by the following expression:
\begin{gather}
\bigg|\frac{\partial^k{\hat{x}}}{\partial{\xi^k}}\bigg| \leq C \int |u|^k|x(u)|du \label{eq:regf}
\end{gather}

On infinite domains, the integrability of the left hand of \eqref{eq:regf} requires the summing integral to be well defined, and thus imposes $x$ to be compactly supported, or every $|u|^k$ to be matched by the spatial decrease of $|x(u)|$ when $u$ diverges.

However this argument does not need to hold on finite domains as finite sums are always defined, so it is not clear whether $\ref{cla:smooth}$ is true for any graph.

\begin{proposition} If $G$ is a complete graph, then \claref{cla:smooth} is false.
\end{proposition}

\begin{proof}
Consider a well chosen construction of $\cu$.
\end{proof}

\paragraph{Define spatial decay, try to prove for grids}

\paragraph{Thinking ...}
Consider two graphs and their laplacians $L_1$ and $L_2$. They define the same spectral conv, iff, $L_1L_2 = L_2L_1$ iff $L_3 = L_1L_2$ is a symmetric matrix \ie a laplacian of another graph.

\paragraph{In fact it is learned smoothness}

\paragraph{Equivariance to $L$}

\todo{}

\paragraph{Polynomial spectral techniques}

\todo{Chebnet Cayleynet}

\subsection{Vertex domain techniques}

\subsection{Others}

