\section{Related works}

\todo{Presentation and note on older graph neural network models}


%\subsection{Methodology}

\subsection{Analysis of spectral techniques}

Given a graph $\gve$, spectral techniques are based on the graph Fourier transform (GFT)~\citep{chung1996spectral,shuman2013emerging}, derived from the laplacian matrix of $G$. Spectral CNNs make use of a convolution defined as pointwise multiplication in the spectral domain defined by the GFT. Neural networks based on such methods were first introduced by \cite{bruna2013spectral}. \cite{henaff2015deep} later extended them to large scale classification tasks, and investigate the problem of estimating a suitable $G$ from data.

Let a graph $\gve$ of order $n$, with adjacency matrix $A$, degree matrix $D$, and let $L$ denote either its normalized laplacian matrix or its unnormalized laplacian matrix. As a real symmetric matrix, $L$ is diagonalized as $L = U^{\T} \Lambda U$, where columns of $U$ form the eigenvectors basis $\cu$ and $\Lambda = \diag(\lambda_1, \ldots , \lambda_n)$ is a diagonal matrix of the corresponding eigenvalues. 

\begin{definition}\textbf{Graph Fourier transform}\\
The \emph{GFT}, \wrt laplacian eigenbasis $\cu$, of a signal defined on~$G$, represented by a column vector $x \in \cs(G)$, is defined as $\cf(x) = Ux$.
\end{definition}

The spectral convolution can then be constructed by similarity with the convolution theorem from classical Fourier analysis~\citep{wiki:conv}.

\begin{definition}\textbf{Spectral convolution}\\
The spectral convolution between two signals $x_1, x_2 \in \cs(G)$ is defined as
\begin{align*}
x_1 \star x_2 & = U^{\T}(Ux_1 \circ Ux_2)\\
 & = U^{\T} \diag(Ux_1) \h{2} Ux_2
\end{align*}
where here $\circ$ denotes Hadamard product and $\forall v \in \bbr^n, \diag(v) = \diag(v[1], \ldots, v[n])$.
\end{definition}

This gives rize to a class of convolution layers. In the following definition we reinterpret them with the formalism introduced in \secref{sec:nn}.

\begin{definition}\textbf{Spectral convolution layer with $\co(n)$ weights}\\
A spectral convolution layer $(g,h)$ is such that its connectivity matrix is of the form:
\begin{gather*}
W_g = U^{\T} \diag(\theta) \h{2} U
\end{gather*}
where $\theta$ contains $n$ learnable weights.
\end{definition}

\begin{remark}Spectral convolution layer are also extended with feature maps and input channels. In fact $W_g := W_g^{p,q}$ where $W_g^{p,q}$ is the $(p,q)$ block corresponding to input channel $p$, and feature map $q$, of a larger connectivity matrix. But we omit the superscript for the sake of simplicity.
\end{remark}

Such layers have two main drawbacks:
\begin{enumerate}
  \item they produce $\co(n)$ weights instead of $\co(1)$ as in the case of classical two-dimensional convolutions,
  \item connections of $W_g$ do not depend on a locality notion based on the original adjacency matrix $A$.
\end{enumerate}

Hence, \cite{bruna2013spectral} suggest to alleviate these issues with the following construction (reinterpreted with our formalism):

\begin{definition}\textbf{Spectral convolution layer with $\co(1)$ weights}\\
A spectral convolution layer $(g,h)$ is such that its connectivity matrix is of the form:
\begin{gather*}
W_g = U^{\T} \diag(\omega) \h{2} U
\end{gather*}
where $\omega$ is obtained from smooth interpolation between a weight vector $\theta$ of size $r = \co(1)$ and a smoothing kernel $K \in \bbr^{n \times r}$ ie $\omega = K \theta$.
\end{definition}

In particular, \cite{bruna2013spectral}, argue that the second issue is answered by the fact that ``in order to learn a layer in which features will be not only shared across locations but also well localized in the original domain, one can learn spectral multipliers which are smooth''. An argument that was also taken up by \cite{henaff2015deep}. This argument is suggested by similarity with the classical Fourier transform for which spatial decay relates with smoothness in the spectral domain. Although this argument is true for a grid of infinite size (corresponding to the case of the discrete Fourier transform (DFT)), it is not necessarily true for a general graph in finite settings.

More precisely, as mentioned by \cite{henaff2015deep}, this argument in classical Fourier analysis is grounded by the following expression:
\begin{gather}
\bigg|\frac{\partial^k{\hat{x}}}{\partial{\xi^k}}\bigg| \leq C \int |u|^k|x(u)|du \label{eq:regf}
\end{gather}

On infinite domains, the integrability of the left hand of \eqref{eq:regf} requires the summing integral to be well defined, and thus imposes $x$ to be compactly supported, or every $|u|^k$ to be matched by the spatial decrease of $|x(u)|$ when $u$ diverges. This argument doesn't need to hold on finite domains as finite sums are always defined.

\begin{claim} (False) Smooth multipliers in the graph spectral domain produce a spatially localized operator in the vertex domain.
\end{claim}

\begin{proof}[Refutation] In our formalism, spatial localization can be understood through the connectivity matrix $W_g$. Hence, to refute the claim, there suffices to notice that as $L$ and $L^n$ commutes, they can be simultaneously diagonalized in a common eigenbasis $\cu_{\C}$. Thus they also define a common GFT \wrt $\cu_{\C}$ along a common spectral convolution. However, their corresponding adjacency matrices $A = D - L$ and $A_n = \diag(L^n) - L^n$ encapsulate notions of locality that can be very dissimilar for some graphs (for example for a rectangular grid graph of finite sizes).
\end{proof}

\paragraph{In fact it's learned smoothness}

\paragraph{Equivariance to $L$}

\todo{}

\paragraph{Polynomial spectral techniques}

\todo{Chebnet Cayleynet}

\subsection{Vertex domain techniques}

\subsection{Others}

