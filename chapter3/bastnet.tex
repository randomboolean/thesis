\section{Translation-convolutional neural networks}
%\section{Extending CNNs using EC symmetries on graph domains}

\subsection{Methodology}


Our method is based on~\cite{pasdeloup2017convolutional}, where the authors have introduced a way to infer a graph from training signals, then translations from the obtained graph to design ad-hoc CNNs. We extend this approach and design strided convolutions along graph downscaling, data augmentation and convolutions on downscaled graphs. \figref{fig:outline} depicts the proposed method.

\input{chapter3/fig_tcnn}

\subsection{Background}

Define a graph $G = \langle V, E \rangle$ with $V$ the set of vertices, and $E \subseteq\binom{V}{2}$ the set of edges. We suppose the graph is connected, as conversely the process can be applied to each connected component of $G$. We denote by $d$ the max degree of the graph and $n = |V|$ the number of vertices.

The authors of~\cite{pasdeloup2017convolutional} propose to inductively define translations as functions from vertices to vertices as follows:

\begin{definition}\textbf{Candidate-translation}\\
  A \emph{candidate-translation} is a function $\phi: U \to V$, where $U \subset V$ and such that:
  \begin{itemize}[noitemsep,nolistsep]
  \item $\phi$ is \emph{injective}:\\
  $\forall v,v' \in U, \phi(v) = \phi(v') \Rightarrow v = v',$
  \item $\phi$ is \emph{edge-constrained}:\\
  $\forall v \in U, (v,\phi(v)) \in E,$
  \item $\phi$ is \emph{strongly neighborhood-preserving}:\\
  $\forall v,v' \in U, (v,v')\in E \Leftrightarrow (\phi(v),\phi(v')) \in E.$
  \end{itemize}
\end{definition}

The cardinal $|V-U|$ is called the \emph{loss} of $\phi$.
Two candidate-translations $\phi$ and $\phi'$ are said to be \emph{aligned} if $\exists v\in U, \phi(v) = \phi'(v)$.
We define $N_r(v)$ as the set of vertices that are at most $r$-hop away from a vertex $v \in V$.\\

\begin{definition}\textbf{Translation}\\
  A \emph{translation} in a graph $G$ is a candidate-translation such that there is no aligned translation with a strictly smaller loss, or is the identity function.
\end{definition}

Note that if the graph is a 2D grid, obtained translations are exactly natural translations on images~\cite{GrePasViaGri201610}.\\

\begin{definition}\textbf{Local translation}\\
A \emph{local translation} of center $v \in V$ is a translation in the subgraph of $G$ induced by $N_2(v)$, that has $v$ in its definition domain.
\end{definition}

As local translations can't be used to design data augmentation and convolutions on downscaled graphs, we also design proxies to global translations.

\begin{definition}\textbf{Proxy-translations}\\
A family of \emph{proxy-translations} $(\psi_p)_{p=0,..\kappa-1}$ initialized by $v_0 \in V$ is defined algorithmically as follows:
\begin{enumerate}
\item We place an indexing kernel on $N_1(v_0) $ i.e.\\ $N_1(v_0) = \{v_0, v_1, ..., v_{\kappa-1}\}$ with $\forall p, \psi_p(v_0) = v_p$,
\item We move this kernel using each local translation $\phi$ of center $v_0$: $\forall p, \psi_p(\phi(v_0)) = \phi(v_p)$,
\item We repeat 2) from each new center reached until saturation. If a center is being reached again, we keep the indexing that minimizes the sum of losses of the local translations that has lead to it.
\end{enumerate}
\end{definition}

\subsection{Efficiently finding translations}
Finding translations is an NP-complete problem~\cite{pasdeloup2017translations}, such that for large graphs the method is not suitable. 
In order to break down complexity, the authors of~\cite{pasdeloup2017convolutional} propose to search for local translations. They also introduce approximate translations which we omit for the sake of simplicity, but the description would be similar. We describe in three steps how we efficiently find proxy-translations.

\paragraph{First step: finding local translations}

For each vertex $v \in G$, we identify all local translations using a bruteforce algorithm. This process requires finding all translations in all induced subgraphs. There are $n$ such subgraphs, each one contains at most $d$ local translations. Finding a translation can be performed by looking at all possible injections from 1-hop vertices around the central vertex to any vertex that is at most 2-hops away. We conclude that it requires at most $\mathcal{O}(nd d^{2(d+1)})$ elementary operations and is thus linear with the order of the graph. On the other hand, it suggests that sparsity of the graph is a key criterion in order to maintain the complexity reasonable.

\figref{fig:gridgraph} depicts an example of a grid graph and the induced subgraph around vertex $v_0$. \figref{fig:inducedtranslations} depicts all obtained translations in the induced subgraph.

\input{chapter3/fig_gridgraph}
\input{chapter3/fig_inducedtranslations}

\paragraph{Second step: using local translations to move a small localized kernel around $G$}

Given an arbitrary\footnote{In practice we run several experiments while changing the initial vertex and keep the best obtained result.} vertex $v_0 \in V$, we place an indexing kernel on $N_1(v_0)$ i.e. $N_1(v_0) = \{v_0, v_1, ..., v_{\kappa-1}\}$. Then we move it using every local translations of center $v_0$, repeating this process for each center that is reached for the first time. We stop when the kernel has been moved everywhere in the graph. In case of multiple paths leading to the same destination, we keep the indexing that minimizes the sum of loss of the series of local translations. We henceforth obtain an indexing of at most $\kappa$ objects of $N_1(v)$ for every $v \in V$.

This process is depicted in \figref{fig:movekernel}. Since it requires moving the kernel everywhere, its complexity is $\mathcal{O}(n d^2)$.

\input{chapter3/fig_movekernel}

\paragraph{Final step: identifying proxy-translations in $G$}

Finally, by looking at the indexings obtained in the previous step, we obtain a family of proxy-translations defined globally on $G$. More precisely, each index defines its own proxy-translation. Note that they are not translations because only the local properties have been propagated through the second step, so there can exist aligned candidates with smaller losses. Because of the constraint to keep the paths with the minimum sum of losses, they are good proxies to translations on $G$.

An illustration on a grid graph is given in \figref{fig:translations}. The complexity is $\mathcal{O}(nd)$. Overall, all three steps are linear in $n$.

\input{chapter3/fig_translations}

\subsection{Translation-convolution}

Let $(\psi_p)_{p=0,..,\kappa-1}$ be the proxy-translations identified on $G$ with the convention that $\psi_0 = id$ is the identity function, and where $\kappa$ is the number of weights in the indexing kernel.

The operation of the \emph{extended convolution layer} centered on the vertex $v \in V$ is defined as:

$$\mathbf{y}_v = h\left(\sum_{p=0}^{\kappa-1}{w_p \mathbf{x}_{\phi_p(v)}} + b\right)$$

where $h$ is the activation function, $b$ is the bias term, $\mathbf{x}_\bot = 0$ and:
$$
\left\{
    \begin{array}{ll}
        \phi_p(v) = \psi_p(v) & \mbox{ if } \psi_p \mbox{ is defined on } v \\
        \phi_p(v) = \bot \notin V & \mbox{ else} \\
    \end{array}
\right..
$$

Note that we defined convolution layers using the formalism of proxy-translations, but they can also be defined using only the formalism of local translations~\cite{pasdeloup2017convolutional}.

%Discussion with chapter 2

\subsection{Subsampling}

Downscaling is a tricky part of the process because it supposes one can somehow regularly sample vectors. As a matter of fact, a nonregular sampling is likely to produce a highly irregular downscaled graph, on which looking for translations irremediably leads to poor accuracy, as we noticed in our experiments.% Instead, here we propose to use translations on $G$ to define the extended downscaling layers. They basically are obtained by masking vertices in the initial graph.

We rather define the translations of the strided graph using the previously found proxy-translations on $G$.\\

\noindent\textbf{First step: extended convolution with stride $r$}
%\subsubsection{First step: convolution with stride $r$}
%Here we define an extended convolution layer with stride $r$.

Given an arbitrary initial vertex $v_0 \in V$, the set of kept vertices $V_{\downarrow r}$ is defined inductively as follows:
\begin{itemize}[noitemsep,nolistsep]
\item $V_{\downarrow r}^0 = \{v_0\}$,
\item $\forall t \in \mathbb{N}, V_{\downarrow r}^{t+1} = V_{\downarrow r}^t \cup \{v \in V, \forall v' \in V_{\downarrow }^t, v \not\in N_{r-1}(v') \land \exists v' \in V_{\downarrow r}^t, v \in N_{r}(v') \}$.
\end{itemize}

This sequence is nondecreasing and bounded by $V$, so it eventually becomes stationary and we obtain $V_{\downarrow r} = \lim_t{V_{\downarrow r}^t}$. Figure~\ref{downscaling} illustrate the first downscaling $V_{\downarrow 2}$ on a grid graph. %% $V_{\downarrow 2}$ is used to define the stride in extended convolution layers.

The output neurons of the extended convolution layer with stride $r$ are $V_{\downarrow r}$.

\paragraph{Second step: convolutions for the strided graph}

Using the proxy-translations on $G$, we move a localized $r$-hop indexing kernel over $G$. At each location, we associate the vertices of $V_{\downarrow r}$ with indices of the kernel, thus obtaining what we define as induced $_{\downarrow r}$-translations on the set $V_{\downarrow r}$. In other words, when the kernel is centered on $v_0$, if $v_1 \in V_{\downarrow r}$ is associated with the index $p_0$, we obtain $\phi_{p_0}^{\downarrow r}(v_0) = v_1$. Subsequent convolutions at lower scales are defined using these induced $_{\downarrow r}$-translations similarly to Subsection C.
%The downstream convolution's weight-sharing schemes are then obtained by moving a kernel covering the full graph to each vertex in $V_{\downarrow r}$, similarly to Subsection C.

%% Note that every downscaling layers of the extended architecture are defined from the translation found on the graph at the input level of the network. A typical extended CNN architecture would contain ${2^r}$ downscaled layers.

\input{chapter3/fig_downscaling}

\subsection{Data augmentation}

Once translations are obtained on $G$, one can use them to move training vectors, artificially creating new ones. Note that this type of data-augmentation is poorer than for images since no flipping, scaling or rotations are used.

\subsection{Matching CNNs on image classification}

On the CIFAR-10 dataset, our models are based on a variant of a deep residual network, namely PreActResNet18\cite{he2016identity}. We tested different combinations of graph support and data augmentation. For the graph support, we use either a regular 2D grid or either an inferred graph obtained by keeping the four neighbours that covary the most. \tabref{tab:cifar-table} summarizes out results. In particular, it is interesting to note that results obtained without any structure prior (91.07\%) are only 2.7\% away from the baseline using classical CNNs on images (93.80\%). This gap is even smaller (less than 1\%) when using the grid prior. Also, without priors our method significantly outperforms the others. %% In the top left corner of the table we fully exploit the fact that the dataset is composed of 2D structured data, and in the bottom right corner this information is not used at all. Even with that disadvantage, the results differ of only 2.7\%, which shows the strenght of the method when compared to others.

\begin{table}
\begin{center}
\caption{CIFAR-10 result comparison table.}
\vspace{-0.4cm}
\label{tab:cifar-table}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Support} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}MLP \\ \cite{LinMK15}\end{tabular}} & \multirow{2}{*}{CNN} & \multicolumn{2}{c|}{Grid Graph (Given)}       & \multicolumn{2}{c|}{Covariance Graph (Inferred)}         \\ \cline{4-7} 
                         &                                                                                &                      & \cite{defferrard2016convolutional} & Proposed & Proposed             & \cite{pasdeloup2017convolutional} \\ \hline
Full Data Augmentation   & 78.62\%                                                                        & \textbf{93.80\%}     & 85.13\%                            & 93.94\%  & 92.57\%              & ------                            \\ \hline
Data Augmentation - Flip & ------                                                                         & 92.73\%              & 84.41\%                            & 92.94\%  & 91.29\%              & ------                            \\ \hline
Graph Data Augmentation  & ------                                                                         & 92.10\%$^a$          & ------                             & 92.81\%  & \textbf{91.07\%}$^b$ & ------                            \\ \hline
None                     & 69.62\%                                                                        & 87.78\%              & ------                             & 88.83\%  & 85.88\%$^b$           & 82.52\%                           \\ \hline
\end{tabular}

}
\vspace{-0.2cm}
\end{center}
\footnotesize{$^a$ As the CNN does not have a graph support we used the covariance graph as support for the graph data augmentation.\\ $^b$ No priors about the structure}\\
\vspace{-.6cm}
\end{table}

\subsection{Experiments on fmri datasets}

The PINES dataset consists of fMRI scans on 182 subjects, during an emotional picture rating task\cite{chang2015sensitive}. We fetched individual first-level statistical maps (beta images) for the minimal and maximal ratings from \url{https://neurovault.org/collections/1964/}, to generate the dataset. Full brain data was masked on the MNI template and resampled to a 16mm cubic grid, in order to reduce dimensionality of the dataset while keeping a regular geometrical structure. Final volumes used for classification contain 369 signals for each subject and rating. 

We used a shallow network. The results on \tabref{tab:iaps-table} show that our method was able to improve over CNNs, MLPs and other graph-based extended convolutional neural networks.

\begin{table}[h]
\centering
\caption{PINES fMRI dataset accuracy comparison table.}
\label{tab:iaps-table}
\begin{tabular}{|l||c|c||c|c|}
\hline
\multicolumn{1}{|l||}{Graph} & \multicolumn{2}{c||}{None} & \multicolumn{2}{c|}{Neighborhood Graph}     \\ \hline
Method                      & MLP & CNN (kernel 1x1)                                 & \cite{defferrard2016convolutional} & Proposed                   \\ \hline
Accuracy                    & 82.62\% & 84.30\%                            & 82.80\%                            & \textbf{85.08\%} \\ \hline
\end{tabular}
\vspace{-.4cm}
\end{table}

