\section*{Chapter overview}

Our goal in this chapter is to understand how neural networks can be extended to other domains than what they were intended for, and in particular to graph domains. To this end, in \secref{sec:rep}, we make an effort to interpretate the linear algebra that underpins a layer to build our intuition. First we state the obvious by explaining further the interpretation of tensor space as a neural space, as well as how to juggle between tensors and signals. Then, we propose a representation based on graphs. Between two layers, a propagation graph explains how the propagation is done. On the input layer, the neurons can have an underlying graph structure. We show that these graphs can be the same one, if and only if, the local receptive fields of the neurons are intertwined. By introducing the notion of weight sharing in our analysis, we discover that a layer on any domain can be expressed by a ternary operation, that we call \emph{neural contraction}. Its operands are the \emph{input signal} $X$, the \emph{weight kernel} $\Theta$, and the \emph{weight sharing scheme} $S$. We denote $Y = h\left(\wideparen{\Theta S X}\right)$, where $Y$ is the \emph{output signal} and $h$ is the \emph{activation function}. In \secref{sec:ternary}, we study this ternary representation. We see that it is generic. We show that under sparse priors, it can be implemented efficiently better than with sparse classes of common libraries by adopting an approach based on local receptive fields. With an experiment based on the ternary representation, we study how exploiting symmetries is beneficial, which justifies the use of convolutions. In \secref{sec:learningscheme}, we study the effect of learning how the weights are shared during the propagation, which amounts to learn two operands in the ternary representation. We explore this avenue for graph domains, with experiments on grid and covariance graphs. Finally, in \secref{sec:trans}, we investigate an exemple of a CNN architecture used for graph signals. The convolution is based on translations on graphs which define the weight sharing scheme $S$ of the convolutional layer. We present the model of translations and the approximation we use, the subsampling layer, the data augmentation, and experiments.
