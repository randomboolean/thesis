\subsection{Graph structures of neural layers}

Recall our notations. Let $\cl = (g,h)$ a neural network layer, where $g: I \rightarrow O$ is its linear part, $h : O \rightarrow O$ is its activation function, $I$ and $O$ are its input and output spaces, which are tensor spaces. Recall from \defref{def:tensor}, that a tensor space has been defined such that its canonical basis is a cartesian product of canonical bases of vector spaces. Let $I = \displaystyle \bigotimes_{k=1}^p \bbv_k$ and $O = \displaystyle \bigotimes_{l=1}^q \bbu_l$.
Their canonical bases are denoted $\vv_k = (\vv_k^1, \ldots, \vv_k^{n_k})$ and $\uu_l = (\uu_l^1, \ldots, \uu_l^{n_l})$.

\begin{definition}\textbf{Topological space and topological layer}\\
We call \emph{topological space} a set of points with their corresponding neighborhoods (which are sets belonging to a class of sets that is closed under intersection), and we call \emph{topological layer}, a layer $\cl$ such that some~$\vv_k$ and some~$\uu_l$ are topological spaces.
\end{definition}

Without loss of generality, we'll assume that topological bases of a topological layer have higher index.

\begin{definition}\textbf{Neuron}
Let a topological layer $\cl$, with topological bases $\vv_{p_1}, \ldots, \vv{p_d}$

\end{definition}



%\begin{definition}\textbf{Topological layer}\\
%We say that $\cl$ is a topological layer, if some $\vv_k$ and some $\uu_l$ are topological spaces.% In that case, we confound the signal spaces defined over them with their respective representational tensor spaces, and we denote $I = \cs(\cb_I)$ and $O = \cs(\cb_O)$.
%\end{definition}

%topo layer only as a remark
%Def Neural signal space then simplification wo loss gen

 Hence, any layer is in fact a topological layer, which is characterized by a propagation graph, as depicted on \figref{fig:pgraph}.

\begin{figure}[H]
\centering
\begin{tikzpicture}
\draw (0,0) -- (4,0) -- (4,4) -- (0,4) -- (0,0);
\node at (2,2){placeholder};
\end{tikzpicture}
\caption{A propagation graph}
\label{fig:pgraph}
\end{figure}

\begin{lemma}\textbf{Propagation graph}\\
Let a layer $(g,h)$ with connectivity matrix $W$ of shape $n \times m$. The connectivity matrix $W$ defines a topological layer such that
\begin{enumerate}
  \item $\ct_O = (\cb_O, \cn_O)$, where $\cn_O = \{ \cn_j = \{ e^I_i, W[i,j] \neq 0 \}, j \in \seq{n}\}$
  \item $\ct_I = (\cb_I, \cn_I)$, where $\cn_I = \{ \cn_i = \{ e^O_j, W[i,j] \neq 0 \}, i \in \seq{m}\}$
\end{enumerate}
We define the \emph{propagation graph} $P = \langle \cb^I, \cb^O, E \rangle$ as the weighted bipartite graph characterized by $W$ as its adjacency matrix. Then $P$ also characterizes the topological spaces $\ct_0$ and $\ct_I$.
\end{lemma}

In the general case of partially connected layer, the latent canonical bases $\cb_I$ and $\cb_O$ are not the same. However, forthe example of convolutional layers, we can adopt an isomorphic point of view $O \cong I$, as we can obtain $\ct_O \cong \ct_I$ (if necessary with padding, and by setting the neighborhoods of the pads to $\emptyset$). Therefore, we can define the underlying graph structure as follows.

\begin{definition}\textbf{Underlying graph structure}\\
Let a topological layer $(g,h)$ such that $I = \cs(\cb_I) \cong O = \cs(\cb_O))$. By identifying their signal domains $V := \cb_I \cong \cb_O$, we define the \emph{underlying graph structure} $\gve$, such that $i \sim j \Leftrightarrow e^I_i \in \cn_j$.
\end{definition}

That is, when a convolution is defined over a graph like in the previous chapter (todo:insert ref here), its underlying graph structure is obviously a subgraph of the graph itself. Nonetheless, this definition also implies that every topological layer where $I \cong O$ is underpinned by a convolution on an underlying graph structure.

For instance:
\begin{itemize}
  \item The underlying graph structure of classical $2$d convolutions is a lattice graph. They can be redefined equivalently as a Cayley convolution on this lattice graph.
  \item The underlying graph structure of a dense layer is a complete graph. The matrix multiplication can be redefined equivalently as a partial convolution without $\varphi$-equivalence (\ie with no equivariance characterization).
\end{itemize}

\figref{upgraph} depicts a underlying graph strucuture basing a convolution, and the corresponding propagation graph.

\begin{figure}[H]
\centering
\begin{tikzpicture}
\draw (0,0) -- (4,0) -- (4,4) -- (0,4) -- (0,0);
\node at (2,2){placeholder};
\end{tikzpicture}
\caption{Underlying graph Vs Prop graph}
\label{fig:upgraph}
\end{figure}