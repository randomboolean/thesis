\section{Layer representations}

Let $\cl = (g,h)$ a neural network layer, where $g: I \rightarrow O$ is its linear part, $h : O \rightarrow O$ is its activation function, $I$ and $O$ are its input and output spaces, which are tensor spaces. 

\subsection{Neural interpretation of tensor spaces}

Recall from \defref{def:tensor} that a tensor space has been defined such that its canonical basis is a cartesian product of canonical bases of vector spaces. Let $I = \bigotimes_{k=1}^p \bbv_k$ and $O =  \bigotimes_{l=1}^q \bbu_l$.
Their canonical bases are denoted $\vv_k = (\vv_k^1, \ldots, \vv_k^{n_k})$ and $\uu_l = (\uu_l^1, \ldots, \uu_l^{n_l})$.

\begin{remark}
Note that a tensor space is isomorph to the signal space defined over its canonical basis.
\end{remark}

More precisely, we have the following relation.

\begin{lemma}\textbf{Relation between tensor and signal spaces}\\
Let $\bbv$, $\bbu$ vector spaces, and $\vv$, $\uu$ their canonical bases. Let $\bbt$ a tensor space. $\otimes$ and $\times$ denote tensor and cartesian products. Then,
\begin{enumerate}[label=(\roman*)]
  \item $\bbv \cong \cs(\vv)$
  \item $\bbv \otimes \bbu \cong \cs(\vv \times \uu)$
  \item $\bbv \otimes \bbt \cong \cs_{\bbt}(\vv)$
\end{enumerate}
where $\cs_{\bbu}$ are signals taking values in $\bbu$ (and $\cs$ are real-valued signals).
\label{lem:relts}
\end{lemma}
\begin{proof}
\begin{enumerate}[label=(\roman*)]
  \item Given $x \in \bbv$, define $\widetilde{x} \in \cs(\vv)$ such that $\forall i, \widetilde{x}[\vv^i] = x[i]$. The mapping $x \mapsto \widetilde{x}$ is a linear isomorphism. \label{enum:li}
  \item $\widetilde{x}[\vv^i, \uu^j] = x[i,j]$
  \item $\widetilde{x}[\vv^i] = x[i,:,\ldots,:]$
\end{enumerate}
\end{proof}

Denote $V = \bigtimes_{k=1}^d \vv$ and $U = \bigtimes_{k=1}^e \uu$. Therefore, we can identify the input and output spaces as $I = \cs(V) \otimes \bigotimes_{k=d+1}^p \bbv_k$ and $O =  \cs(U) \otimes \bigotimes_{l=e+1}^q \bbu_l$. As $\cs(\vv) \otimes \bbt = \cs_{\bbt}(\vv)$, an object of $V$ or $U$ can be interpreted as the representation of a \emph{neuron} which can take multiple values.

In what follows, without loss of generality, we will make the simplification that a neuron can only take a single value (we don't consider input channels and feature maps yet). We'll thus consider that $I = \cs(V)$ and $O = \cs(U)$. $V$~is the set of \emph{input neurons}, and $U$ is the set of \emph{output neurons}.

\subsection{Graph structure of the propagation}

Let $\cl = (g,h)$, recall that $g: \cs(V) \rightarrow \cs(U)$ is characterized by a connectivity matrix $W$ such that, $g(x) = Wx$.

\begin{remark}Using the mapping defined in the proof of \lemref{lem:relts}, for notational conveniency, we'll abusively consider $x$ as a vector (enventually reshaped from a tensor), and $W$ as a binary tensor product for its indexing (\ie $W[u,v] := W[i,j]$ where $u = \uu^i$ and $v = \vv^j$).
\end{remark}

% Define bipartite graph here?

\begin{definition}\textbf{Propagation graph}\\
The \emph{propagation graph} $P$ of a layer $\cl = (W,h)$ is the bipartite graph that has the connectivity matrix $W$ for bipartite adjacency matrix.
\end{definition}

An example is depicted on \figref{fig:pgraph}.

\begin{figure}[H]
\centering
\begin{tikzpicture}
\draw (0,0) -- (4,0) -- (4,4) -- (0,4) -- (0,0);
\node at (2,2){placeholder};
\end{tikzpicture}
\caption{A propagation graph}
\label{fig:pgraph}
\end{figure}

% This representation stresses out the fact that every layer is in fact a partially connected layer.
% To say in weight sharing scheme

The propagation graph defines an input topological space~$\ct_V$, and an output topological space~$\ct_U$.

\begin{definition}\textbf{Topological space}\\
A \emph{topological space} is a pair $\ct = (X, \co)$, where $X$ is a set of points, $\co$ is a set of sets that is closed under intersection (the \emph{open sets}), and such that every point $x \in X$ is associated with a set $\cn_x \in \co$, called its \emph{neighborhood}.
\end{definition}

Hence, the \emph{neural topologies} $\ct_V$ and $\ct_U$ are defined as
\begin{enumerate}
\item $\ct_V = (V, \co(U))$, with $\forall v \in V, \cn_v = \{u \in U, u \overset{P}\leftarrow v \}$
\item $\ct_U = (U, \co(V))$, with $\forall u \in U, \cn_u = \{v \in V, v \overset{P}\rightarrow u \}$
\end{enumerate}

\subsection{Underlying graph structure}

If the input and output spaces of a layer are topologically isomorph (in the sense that the open sets and the neighborhoods are preserved by a bijective mappring \ie $u \overset{P}\leftarrow v \Leftrightarrow \widetilde{u} \overset{P}\rightarrow \widetilde{v}$), then we can identify output neurons with input neurons $U:=V$. In that case, the propagation is supported \emph{on} an underlying graph structure, that is not a bipartite graph (contrary to the propagation graph).

\begin{definition}\textbf{Underlying graph structure}\\
Let a layer $\cl: \cs(V) \rightarrow \cs(U)$ such that $\ct_V \cong \ct_U$. Its \emph{underlying graph structure} is the graph $\gve$ such that $E = \{ \{u,v\} \in V^2, \widetilde{u} \in \cn_v \}$.
\end{definition}



\todo{}









% \begin{lemma}\textbf{Propagation graph}\\
% Let a layer $(g,h)$ with connectivity matrix $W$ of shape $n \times m$. The connectivity matrix $W$ defines a topological layer such that
% \begin{enumerate}
%   \item $\ct_O = (\cb_O, \cn_O)$, where $\cn_O = \{ \cn_j = \{ e^I_i, W[i,j] \neq 0 \}, j \in \seq{n}\}$
%   \item $\ct_I = (\cb_I, \cn_I)$, where $\cn_I = \{ \cn_i = \{ e^O_j, W[i,j] \neq 0 \}, i \in \seq{m}\}$
% \end{enumerate}
% We define the \emph{propagation graph} $P = \langle \cb^I, \cb^O, E \rangle$ as the weighted bipartite graph characterized by $W$ as its adjacency matrix. Then $P$ also characterizes the topological spaces $\ct_0$ and $\ct_I$.
% \end{lemma}

% In the general case of partially connected layer, the latent canonical bases $\cb_I$ and $\cb_O$ are not the same. However, for the example of convolutional layers, we can adopt an isomorphic point of view $O \cong I$, as we can obtain $\ct_O \cong \ct_I$ (if necessary with padding, and by setting the neighborhoods of the pads to $\emptyset$). Therefore, we can define the underlying graph structure as follows.

% \begin{definition}\textbf{Underlying graph structure}\\
% Let a topological layer $(g,h)$ such that $I = \cs(\cb_I) \cong O = \cs(\cb_O))$. By identifying their signal domains $V := \cb_I \cong \cb_O$, we define the \emph{underlying graph structure} $\gve$, such that $i \sim j \Leftrightarrow e^I_i \in \cn_j$.
% \end{definition}

% That is, when a convolution is defined over a graph like in the previous chapter (todo:insert ref here), its underlying graph structure is obviously a subgraph of the graph itself. Nonetheless, this definition also implies that every topological layer where $I \cong O$ is underpinned by a convolution on an underlying graph structure.

% For instance:
% \begin{itemize}
%   \item The underlying graph structure of classical $2$d convolutions is a lattice graph. They can be redefined equivalently as a Cayley convolution on this lattice graph.
%   \item The underlying graph structure of a dense layer is a complete graph. The matrix multiplication can be redefined equivalently as a partial convolution without $\varphi$-equivalence (\ie with no equivariance characterization).
% \end{itemize}

\figref{upgraph} depicts a underlying graph strucuture basing a convolution, and the corresponding propagation graph.

\begin{figure}[H]
\centering
\begin{tikzpicture}
\draw (0,0) -- (4,0) -- (4,4) -- (0,4) -- (0,0);
\node at (2,2){placeholder};
\end{tikzpicture}
\caption{Underlying graph Vs Prop graph}
\label{fig:upgraph}
\end{figure}