\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{color}
\usepackage{enumitem}
\usetikzlibrary{arrows}
\usepackage{framed}
\usepackage{arydshln}
\usepackage{multirow}
\usepackage{mathtools}


% no indent
\setlength\parindent{0pt}

% commands
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{remark}{Remark}

\newcommand{\domain}{\mathcal{D}}
\newcommand{\image}{\mathcal{I}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\ovec}{}

\newcommand{\todo}[1]{\textcolor{red}{TODO: #1\\}}

\title{Structures en apprentissage profond.\\ On structures in deep learning}

\begin{document}

\section{Definitions}

In this section we recall and define the notions we will be using throughout this manuscript.

\subsection{Tensors}

\todo{reformat}

Vector spaces considered in this thesis are always assumed to be finite-dimensional.

\begin{definition}\textbf{Tensor space}\\
We define a \emph{tensor space of rank $r$} as a cartesian product of $r$ vector spaces, equipped with the coordinate-wise sum and the monolinear outer product.
Its \emph{shape} is denoted $n_1 \times n_2 \times \cdots \times n_r$, where the $\{n_k\}$ are the dimensions of the vector spaces.
For notational conveniency, we will often abusively confound vector spaces with tensor spaces of rank $1$, and matrix spaces with tensor spaces of rank $2$.
\end{definition}

\begin{definition}\textbf{Tensor}\\
A \emph{tensor} $t$ is an object of a tensor space.
An \emph{entry} of $t$ is denoted $t[i_1, i_2, \ldots, i_r]$.
The \emph{shape} of the tensor $t$ is denoted $n_1(t) \times n_2(t) \times \cdots \times n_r(t)$
\end{definition}

\begin{definition}\textbf{Slice}\\
A \emph{slice} of a tensor $t$, along the last ranks $\{r_1, r_2, \ldots, r_s\}$, and indexed by  $\{i_{r_1}, i_{r_2}, \cdots, i_{r_s}\}$, is the tensor $t'$ of rank $r-s$ such that
\begin{align*}
t'[i'_1, i'_2, \ldots, i'_{r-s}] &= t[i'_1, i'_2, \ldots, i'_{r-s}, i_{r_1}, i_{r_2}, \ldots, i_{r_s}] \\
\text{ i.e. } \quad t' :&= t[:,:, \ldots, :, i'_{r-s}, i_{r_1}, i_{r_2}, \ldots, i_{r_s}]
\end{align*}
It is denoted $t_{i_{r_1} i_{r_2} \cdots i_{r_s}}$.
\end{definition}

\begin{definition}\textbf{Contraction}\\
\end{definition}

\begin{remark}\textbf{Covariant and contravariant indices}\\
\end{remark}

\begin{definition}\textbf{Convolution}\\
\end{definition}

\begin{definition}\textbf{Flattened space}\\
\end{definition}

\subsection{Neural Networks}

We denote by $I_f$ the \textit{domain of definition} of a function $f$ ("I" for "input") and by $O_f = f(I_f)$ its \textit{image} ("O" for "output"), and we represent it as $I_f~\xrightarrow{f}~O_f$.

An activation function $h$ defined from a tensor space to itself is a $1$-d function applied dimension-wise and we use the functional notation $h(v)[i_1, i_2, \ldots, i_r] = h(v[i_1, i_2, \ldots, i_r])$.

\begin{definition}\textbf{Neural network}\\
\label{nndef}
{Let $F$ be a function such that $I_f$ and $O_f$ are vector or tensor spaces.\\
$F$ is a \emph{functional formulation} of a \emph{neural network} if there are a series of linear or affine functions $(g_k)_{k=1,2,..,L}$ and a series of non-linear derivable activation functions $(h_k)_{k=1,2,..,L}$ such that:
\begin{gather}
\left\{
  \begin{array}{l}
    \forall k \in \{1,2,..,L\}, f_k = h_k \circ g_k, \\
    I_F = I_{f_1} \xrightarrow{f_1} O_{f_1} \cong I_{f_2} \xrightarrow{f_2} \dots \xrightarrow{f_L} O_{f_L} = O_F, \\
    F = f_{L} \circ ... \circ f_{2} \circ f_1
  \end{array}
\right.
\end{gather}
The couple $(g_k, h_k)$ is called the \emph{$k$-th layer} of the neural network.
For $\ovec{x} \in I_f$, we denote by $\ovec{x_k} = f_k \circ ... \circ f_{2} \circ f_1 (\ovec{x})$ the \emph{activations} of the $k$-th layer.
}
\end{definition}

\begin{remark}\textbf{Bias}\\
Affine functions $\widetilde{g}$ can be written as a sum between a linear function $g$ and a constant vector $b$. For notational conveniency, we only consider linear functions in this section. $b$ is called the \emph{bias}. Its role is to augment the expressivity of the neural network's family of functions.
\end{remark}

\begin{remark}\textbf{Connectivity matrix}\\
Any linear function $g$ is characterized by a \emph{set of parameters} $\Theta_g$, that we order arbitrarily in the dimensions of a vector $\theta_g$. Without loss of generality, let's suppose $I_g$ and $O_g$ are vector spaces\footnote{for instance if they are tensor spaces, they can be reshaped to vector spaces}. Then there exists a \emph{connectivity matrix}~$W_g$, for which:
\begin{gather}
\left\{
\begin{array}{l}
  \forall \ovec{x} \in I_g, g(\ovec{x}) = W_g\ovec{x}\\
  \forall (i,j),
    \begin{cases}
      \exists w \in \Theta_g, W_g[i,j] := w \\
      \text{ or } W_g[i,j] = 0
    \end{cases}
\end{array}
\right.
\end{gather}
\end{remark}

\begin{remark}\textbf{Biological inspiration}\\
A \emph{(computational) neuron} is a computational unit that is biologically inspired. Each neuron should be capable of:
\begin{enumerate}
\item receiving modulated signals from other neurons and aggregate them,
\item applying to the result a derivable activation,
\item passing the signal to other neurons.
\end{enumerate}
That is to say, each domain $\{I_{f_k}\}$, $O_F$ can be interpreted as a layer of neurons, with one neuron for each dimension. The connectivity matrices $\{W_k\}$ describe the connexions between each successive layers. A neuron is illustrated on Figure~\ref{fig:neuron}.
\end{remark}

\begin{figure}[h]
\centering
\begin{tikzpicture}
\draw (0,0) -- (4,0) -- (4,4) -- (0,4) -- (0,0);
\end{tikzpicture}
\caption{A neuron (placeholder)}
\label{fig:neuron}
\end{figure}

\begin{definition}\textbf{Weights}\\
The \emph{weights} of the $k$-th layer of a neural network, denoted $\Theta_k$, are defined as the set of parameters of its linear part.
A weight that appears multiple times in $W_k$ is said to be \emph{shared}. Two parameters of $W_k$ that share a weight are said to be \emph{tied}.
\end{definition}

\begin{remark}\textbf{Training}\\
Usually, a \emph{loss} function $\mathcal{L}$ penalizes the output $x_L = F(\ovec{x})$. Its gradient w.r.t.~$\theta_k$, denoted $\vec{\bigtriangledown}_{\theta_k}$, is used to update the weights via an optimization algorithm based on gradient descent and a learning rate $\alpha$, that is:
\begin{gather}
\theta_k^{\text{new}} = \theta_k^{\text{old}} - \alpha \cdot \vec{\bigtriangledown}_{\theta_k}
\end{gather}
where $\alpha$ depends on training variables and can be a scalar or a vector, and so $\cdot$~denotes here outer or pointwise product.
\todo{Give some ref of optimization algorithms}
\end{remark}

\begin{remark}\textbf{Linear complexity}\\
{Thanks to the chain rule, $\vec{\bigtriangledown}_{\theta_k}$ can be computed using gradients that are w.r.t. $\ovec{x_k}$, denoted $\vec{\bigtriangledown}_{x_k}$, which in turn can be computed using gradients w.r.t. outputs of the next layer $k+1$, up to the gradients given on the output layer.

That is:
%$$
%\left\{
%\begin{array}{l}
\begin{align}
  \vec{\bigtriangledown}_{\theta_k} & = J(\ovec{x_k})_{\theta_k} \vec{\bigtriangledown}_{x_k} \\
  \begin{split}
  \vec{\bigtriangledown}_{x_k} & = J(\ovec{x_{k+1}})_{x_k} \vec{\bigtriangledown}_{x_{k+1}} \\
  \vec{\bigtriangledown}_{x_{k+1}} & = J(\ovec{x_{k+2}})_{x_{k+1}} \vec{\bigtriangledown}_{x_{k+2}} \\
  \quad \quad \ldots\\
  \vec{\bigtriangledown}_{x_{L-1}} & = J(\ovec{x_{L}})_{x_{L-1}} \vec{\bigtriangledown}_{x_{L}}
  \label{eq:bp}
  \end{split}
\end{align}
Obtaining,
\begin{align}
  \vec{\bigtriangledown}_{\theta_k} = J(\ovec{x_k})_{\theta_k} (\prod_{p=k}^{L-1} J(\ovec{x_{p+1}})_{x_p}) \vec{\bigtriangledown}_{x_L}
\end{align}
%\end{array}
%\right.
%$$
where $J(.)_{\text{wrt}}$ are the respective jacobians which can be determined with the layer's expressions and the $\{x_k\}$.

This allows to compute the gradients with a complexity that is linear with the number of weights, instead of being quadratic if it were done with the difference quotient expression of the derivatives.}
\end{remark}

\begin{remark}\textbf{Neural interpretation}\\

We can remark that \eqref{eq:bp} rewrites as
\begin{align}
  \begin{split}
  \vec{\bigtriangledown}_{x_k} & = J(\ovec{x_{k+1}})_{x_k} \vec{\bigtriangledown}_{x_{k+1}} \\ 
                               & = J(h(x'_k))_{x'_k} J(W_k x_k)_{x_k} \vec{\bigtriangledown}_{x_{k+1}}
                               %& = \text{diag}( h_k'(W_k x_k) ) W_k \vec{\bigtriangledown}_{x_{k+1}}
  \end{split}
\end{align}
where $x'_k = W_k x_k$, and these jacobians can be expressed as:
\begin{align}
  \begin{split}
  J(h(x'_k))_{x'_k} & [i,j] = \delta_i^j h'(x'_k[i])\\
  J(h(x'_k))_{x'_k} & = I h'(x'_k)
  \end{split}\\
  J(W_k x_k)_{x_k} & = W_k^T
\end{align}
That means that we can write $\vec{\bigtriangledown}_{x_k} = (\widetilde{h}_k \circ \widetilde{g}_k)(\vec{\bigtriangledown}_{x_{k+1}})$ such that the connectivity matrix $\widetilde{W}_k$ is obtained by transposition. This can be interpreted as gradient calculation being a \emph{back-propagation} on the same neural network, in opposition of the \emph{forward-propagation} done to compute the output.
\end{remark}

\begin{definition}\textbf{Dense layer}\\
A \textit{dense layer} $(g,h)$ is a layer such that
$$
\left\{
\begin{array}{l}
  I_g \mbox{ and } O_g \mbox{ are vector spaces} \\
  \forall w \in \Theta_g, \exists! (i, j), W_g[i,j] := w \\
  | \Theta_g | = n_1(W_g)n_2(W_g)
\end{array}
\right.
$$
\end{definition}

\begin{definition}\textbf{Partially connected layer}\\
A \textit{partially connected layer} is a layer such that
$$
\left\{
\begin{array}{l}
  I_g \mbox{ and } O_g \mbox{ are vector spaces}\\
  \exists (i,j), \forall w \in \Theta_g, W_g[i,j] :\neq w
\end{array}
\right.
$$
\end{definition}

\begin{definition}\textbf{Convolutional layer}\\
A \textit{$n$-dimensional convolutional layer} $(g,h)$ is a layer such that there is a \textit{weight tensor} $W$ of rank $n+2$ for which
$$
\left\{
\begin{array}{l}
  I_g \mbox{ and } O_g \mbox{ are tensor spaces of rank }n+1 \\
  \forall x \in I_g, g(x) = (g(x)_q = \sum\limits_p{W_{pq} \ast_n x_p})_{\forall q}
\end{array}
\right.
$$
where $p$ and $q$ index the last ranks and $\ast_n$ denotes the n-d convolution. The tensor slices indexed by $p$ and $q$ are typically called \textit{feature maps}.
Note that if $I_g$ and $O_g$ are reshaped to vector spaces, $W_g$ would be a Toeplitz matrix.
\end{definition}

\begin{definition}\textbf{Pooling}\\
A layer with \textit{pooling} $(g,h)$ is such that $g = g_1 \circ g_2$, where $(g_1,h)$ is a layer and $g_2$ is a pooling operation.
\end{definition}

\begin{definition}\textbf{Reshaping}\\
\end{definition}

A layer with \textit{dropout} $(g,h)$ is such that $h = h_1 \circ h_2$, where $(g,h_2)$ is a layer and $h_1$ is a dropout operation~\cite{srivastava2014dropout}. When dropout is used, a certain number of neurons are randomly set to zero during the training phase, compensated at test time by scaling down the whole layer. This is done to prevent overfitting.

\todo{neuron interpretation}

A multilayer perceptron (MLP)~\cite{hornik1989multilayer} is a neural network composed of only dense layers.
A convolutional neural network (CNN)~\cite{lecun1998gradient} is a neural network composed of convolutional layers.

Neural networks are commonly used for machine learning tasks. For example, to perform supervised classification, we usually add a dense output layer $s=(g_{L+1},h_{L+1})$ with as many neurons as classes. We measure the error between an output and its expected output with a discriminative loss function $\mathcal{L}$. During the training phase, the weights of the network are adapted for the classification task based on the errors that are back-propagated~\cite{hornik1989multilayer} via the chain rule and according to a chosen optimization algorithm (e.g. \cite{bottou2010large}).

\subsection{Graphs}

A graph $G$ is defined as a couple $(V,E)$ where $V$ represents the set of nodes and $E \subseteq\binom{V}{2}$ is the set of edges connecting these nodes.

\todo{Example of figure}

We encounter the notion of graphs several times in deep learning:
\begin{itemize}
\item Connections between two layers of a deep learning model can be represented as a bipartite graph, coined \emph{connectivity graph}. It encodes how the information is propagated through a layer to another. See section~\ref{con_graph}.
\item A computation graph is used by deep learning frameworks to keep track of the dependencies between layers of a deep learning models, in order to compute forward and back-propagation. See section~\ref{comp_graph}.
\item A graph can represent the underlying structure of an object (often a vector), whose nodes represent its features. See section~\ref{inductive_graph}.
\item Datasets can also be graph-structured, where the nodes represent the objects of the dataset. See section~\ref{transductive_graph}.
\end{itemize}

\subsubsection{Connectivity graph}
\label{con_graph}

A Connectivity graph is the bipartite graph whose adjacency matrix is the connectivity graph of a layer of neurons.
%$U = \{u_1, u_2, \ldots, u_n\}$
Formally, given a linear part of a layer, let $\textbf{x}$ and $\textbf{y}$ be the input and output signals, $n$ the size of the set of input neurons $N = \{u_1, u_2, \ldots, u_n\}$, and $m$ the size of the set of output neurons $M = \{v_1, v_2, \ldots, v_m\}$. This layer implements the equation $y = \Theta x$ where $\Theta$ is a $n \times m$ matrix.

\begin{definition}
{The \emph{connectivity graph} $G = (V,E)$ is defined such that $V = N \cup M$ and $E = \{(u_i,v_j) \in  N \times M, \Theta_{ij} \neq 0 \} $.}
\end{definition}

I.e. the connectivity graph is obtained by drawing an edge between neurons for which $\Theta_{ij} \neq 0$.
For instance, in the special case of a complete bipartite graph, we would obtain a dense layer. 
Connectivity graphs are especially useful to represent partially connected layers, for which most of the $\Theta_{ij}$ are $0$. 
For example, in the case of layers characterized by a small local receptive field, the connectivity graph would be sparse, and output neurons would be connected to a set of input neurons that corresponds to features that are close together in the input space. Figure~\ref{con_ex} depicts some examples.

\begin{figure}[h]
  \begin{center}
    \begin{tikzpicture}
      \tikzstyle{every node} = [draw, circle, thick, inner sep = 2pt]
      \foreach \y in {0,...,4}{
        \pgfmathtruncatemacro{\yplusone}{5 - \y}
        \node(a\y) at (0,.6*\y) {\footnotesize\yplusone};
      }
      \foreach \y in {0,...,4}{
        \pgfmathtruncatemacro{\yplusone}{5 - \y}
        \node(\y) at (2,.6*\y) {\footnotesize\yplusone};
      }

      \foreach \x in {0,...,4}{
        \foreach \y in {0,...,4}{
          \path[opacity=0.5] (a\x) edge (\y);
        }
      }
    \end{tikzpicture}
  \end{center}
  \caption{Examples}
  \label{con_ex}
\end{figure}

\todo{Figure~\ref{con_ex}. It's just a placeholder right now}


Connectivity graphs also allow to graphically modelize how weights are tied in a neural layer. Let's suppose the $\Theta_ij$ are taking their values only into the finite set $K = \{w_1, w_2, \ldots, w_\kappa\}$ of size $\kappa$, which we will refer to as the \emph{kernel} of \emph{weights}. Then we can define a labelling of the edges $s: E \rightarrow K$. $s$ is called the \emph{weight sharing scheme} of the layer. This layer can then be formulated as $\displaystyle \forall v \in M, y_v = \sum_{u \in N, (u,v) \in E} w_{s(u,v)} x_u$. Figure~\ref{cnn} depicts the connectivity graph of a 1-d convolution layer and its weight sharing scheme.

\begin{figure}[h]
  \begin{center}
    \begin{tikzpicture}
      \tikzstyle{every node} = [draw, circle, thick, inner sep = 2pt]
      \foreach \y in {0,...,4}{
        \pgfmathtruncatemacro{\yplusone}{5 - \y}
        \node(a\y) at (0,.6*\y) {\footnotesize\yplusone};
      }
      \foreach \y in {0,...,4}{
        \pgfmathtruncatemacro{\yplusone}{5 - \y}
        \node(\y) at (2,.6*\y) {\footnotesize\yplusone};
      }
      \path[opacity=0.5]
      (a0) edge (0);
      \path[dashed]
      (a0) edge (1);
      \path[dotted]
      (a1) edge (0);
      \path[opacity=0.5]
      (a1) edge (1);
      \path[dashed]
      (a1) edge (2);
      \path[dotted]
      (a2) edge (1);
      \path[opacity=0.5]
      (a2) edge (2);
      \path[dashed]
      (a2) edge (3);
      \path[dotted]
      (a3) edge (2);
      \path[opacity=0.5]
      (a3) edge (3);
      \path[dashed]
      (a3) edge (4);
      \path[dotted]
      (a4) edge (3);
      \path[opacity=0.5]
      (a4) edge (4);
    \end{tikzpicture}
  \end{center}
  \caption{Depiction of a 1D-convolutional layer and its weight sharing scheme.}
  \label{cnn}
\end{figure}


\todo{Add weight sharing scheme in Figure~\ref{cnn}}

\subsubsection{Computation graph}
\label{comp_graph}

\subsubsection{Underlying graph structure}
\label{inductive_graph}

\subsubsection{Graph-structured dataset}
\label{transductive_graph}

transductive vs inductive

\subsection{Geometric grids}

\subsection{Grid graphs}

\subsection{Spatial graphs}

\subsection{Projections of spatial graphs}

\end{document}