\section{Convolutions on graphs (draft)}

Defining a convolution on graphs is a challenging problem. Obviously, the underlying structure determined by a graph is not necessarily isomorphic to a set onto which the convolution is already defined. 

Related works: moura, spectral convolution with laplacian.

A convolution may comprise the following properties: bilinear, equivariant with respect to a certain class of isomorphism.

We shall first study classes of graphs onto which the convolution can be naturally defined before generalizing.

*Convolution without the edges

*Convolution on grids

*Convolution on lattice-regular graphs

*Convolution on product graphs

*Convolution on linear combination of circulant graphs

\subsection{Convolution without the edges}

We first consider a grid graph $G = \langle V,E \rangle$ agnostically of its edges \ie $G \cong \bbz^2$. By restriction to compactly supported signals, this case encompass the case of images.

\begin{definition}\textbf{Transformation}\\
A \emph{transformation} $f: V \rightarrow V$ is a function with same domain and codomain.

A transformation can also act on signals by extending their definition to signal space over their domain, $f: \cs(V) \rightarrow \cs(V)$ with the notation
\begin{gather*}
\forall s \in \cs(V), \forall v \in V, f(s)[v] = \sum_{u \in f^{-1}\{v\}}{s[u]}
\end{gather*}
For a bijective transformation $f$, we have simply:
\begin{gather*}
\forall s \in \cs(V), \forall v \in V, f(s)[v] = s[f^{-1}(v)]
\end{gather*}
\end{definition}

In other words, the entry of a transformed signal is obtained by doing a lookup of the entries of the original signal that were transformed into it.

\begin{definition}\textbf{Translation on $\cs(\bbz^2)$}\\
A translation on $\bbz^2$ is defined as a transformation $t: \bbz^2 \rightarrow \bbz^2$ such that
\begin{gather*}
\exists (a,b) \in \bbz^2, \forall (x,y) \in \bbz^2, t(x,y) = (x+a,y+b)
\end{gather*}
It also acts on $\cs(\bbz^2)$ and is denoted $t_{a,b}$ \ie
\begin{gather*}
\forall s \in \cs(\bbz^2), \forall (x,y) \in \bbz^2, t_{a,b}(s)[x,y] = s[x-a, y-b]
\end{gather*}
For any set $E$, we denote by $\ct(E)$ its translations.
\end{definition}

The next proposition can be seen as a discretization of a classic result in distribution theory.

\begin{proposition}\textbf{Characterization of convolution operators on $\cs(\bbz^2)$}\\
On real-valued signals over $\bbz^2$, the class of linear transformations that are equivariant to translations is exactly the class of convolutive operations \ie
\begin{gather*}
\begin{cases}
 f \in \cl(\cs(\bbz^2))\\
 \forall t \in \ct(\cs(\bbz^2)), f \circ t = t \circ f
\end{cases}
 \Leftrightarrow \exists w \in \cs(\bbz^2), f = . \ast w
\end{gather*}
\label{prop:equi}
\end{proposition}

\begin{proof}
The fact that a convolution operator is equivariant to translations is a direct consequence of their definitions. We prove that the converse is also true.
Let $f \in \cl(\cs(\bbz^2))$, $s \in \cs(\bbz^2)$. We suppose that $f$ commutes with translations.

For $(x,y) \in \bbz^2$ we denote by $\delta_{x,y}$ the dirac signal
\begin{gather*}
\delta_{x,y}[i,j] = \begin{cases} 1 & \text{if } (x,y) = (i,j)\\ 0 & \text{otherwise} \end{cases}
\end{gather*}
Then,
\begin{gather*}
s = \displaystyle \sum_i \sum_j s[i,j] \h{2} \delta_{i,j}
\end{gather*}
By linearity of $f$, and equivariantness to translations:
\begin{align*}
f(s) & = \displaystyle \sum_i \sum_j s[i,j] \h{2} f(\delta_{i,j})\\
 & = \displaystyle \sum_i \sum_j s[i,j] \h{2} f \circ t_{i,j} (\delta_{0,0})\\
 & = \displaystyle \sum_i \sum_j s[i,j] \h{2} t_{i,j} \circ f (\delta_{0,0})
\end{align*}
By denoting $w = f (\delta_{0,0}) \in \cs(\bbz^2)$, we obtain:
\begin{align}
\forall (a,b) \in \bbz^2, f(s)[a,b] & = \displaystyle \sum_i \sum_j s[i,j] \h{2} t_{i,j}(w)[a,b] \label{eq:conv}\\
 & = \displaystyle \sum_i \sum_j s[i,j] \h{2} w[a-i, b-j] \nonumber\\
\text{\ie } f(s) & = s \ast w \nonumber
\end{align}
\end{proof}

One important argument in favor of convolutional neural networks is that convolutional layers are equivariant to translations. Intuitively, that means that an object in an image should produce the same features independently of its position in the image. In fact, as a consequence of~\propref{prop:equi} any neural layer that is equivariant to translations is also a convolutional layer. To our knowledge, this converse result has never been discussed in deep learning literature.

% \begin{claim}\textbf{Expressiveness of ReLU-CNNs}\\
% The search space of the feature extraction part of convolutional rectifier neural network is made of piecewise convolutions, and contains every piecewise linear translation equivariant functions.
% \end{claim}

\begin{claim}\textbf{Expressiveness of convolution layer of a neural netowork}\\
The search space of the linear part of a convolutional layer contains every linear translation equivariant functions.
\end{claim}

It shall then be natural that convolutions are constructed from this characterization.

To construct convolution operators on any graph $G$ with this characterization, we note from the former proof that all we need is the definition of the translations $t_{i,j}$, which have no reason whatsoever to be defined naturally on~$G$. More generally, any class of transformations on the vertices that would be entirely determined by their image on a certain vertex would be enough to construct a class of convolution operators. Such transformations would replace the translations $t_{i,j}$ in the construction of a convolution operator in line \eqref{eq:conv} of the previous proof. This give rize to the following definitions.

\begin{definition}\textbf{Grounded set of transformations}\\
A set of transformations over a graph $G = \langle V,E \rangle$, \emph{grounded} on a vertex $v_0 \in V$, denoted $\cp_{v_0} \subset \Phi(V)$, is a set that is in one-to-one correspondence with $V$, such that $\forall v \in V, \exists! p_v \in \cp_{v_0}, p_v(v_0) = v$.
\end{definition}

We have $\cp_{v_0} = \order(G) \in \bbn \cup \{+\infty\}$. For notational convenience we drop the subscript $_{v_0}$ in what follows.

\begin{definition}\textbf{$\cp$-equivariant convolution operator}\\
Let $G = \langle V,E \rangle$ a graph, not necessarily a grid. Let $\cp$ a grounded set of transformations. Then, the  $\cp$-equivariant convolution operator $f_w$ is defined as
\begin{gather*}
\forall s \in \cs(V), f_w(s) = s \ast_{\cp} w = \displaystyle \sum_v s[v] \h{2} p_v(w)
\end{gather*}
\end{definition}

\begin{claim}\textbf{Characterization of $\cp$-eq. convolution operator}\\
The class of linear graph signal transformations that are equivariant to a grounded set $\cp$ is exactly the class of $\cp$-equivariant convolutive operations.
\end{claim}

\begin{proof}
By construction of $\cp$-equivariant convolutions, the proof is similar to the one of \propref{prop:equi}.
\end{proof}



*note on drawbacks. If transformations were a group -> group convolution.
*note on morphisms

\subsection{Special classes of graphs}

% \begin{definition}\textbf{Infinite graph}\\
% An \emph{infinite graph} is defined by natural extension of the notion of graph $G=\langle V,E \rangle$ where $V$ and $E$ can be infinite. We denote $\order{G} = \infty$.
% \end{definition}

\begin{definition}\textbf{Graph automorphisms}\\
A graph automorphism of a graph $G = \langle V,E \rangle$ is a bijection in the vertex domain $\phi: V \rightarrow V$ such that $\{u,v\} \in E \Leftrightarrow \{\phi(u), \phi(v)\} \in E$. We denote $\ca(G)$ the group of automorphism on $G$.

We denote by $\ce(\phi)$ the set of input-output mapping of $\phi$, defined as $\ce(\phi) = \{ (x,y) \in V^2, \phi(x) = y \}$.

A graph automorphism $\phi$ is said to be \emph{edge-constrained} (EC) if $\ce(\phi) \subseteq E$. We denote $\ca_{\EC}(G)$ the set of edge-constrained automorphism on $G$.
\end{definition}

\begin{definition}\textbf{Orthogonality}\\
Two graph automorphisms $\phi_1$ and $\phi_2$ are said to be orthogonal, if and only if $\ce(\phi_1) \cap \ce(\phi_2) = \emptyset$, denoted $\phi_1 \bot \phi_2$. They are said to be aligned otherwise.

Similarly, we define orthogonality of $r$ automophisms as $\phi_1 \bot \cdots \bot \phi_r \Leftrightarrow \ce(\phi_1) \cap \cdots \cap \ce(\phi_r) = \emptyset$
\end{definition}


\subsection{Lattice-regular graph}


\begin{definition}\textbf{Lattice-regular graph}\\
A lattice-regular graph is a regular graph that admits $r$ orthogonal edge-constrained automorphisms, where $r$ is its degree.
\end{definition}


%\subsubsection{Grids}

%\subsubsection{Lattices}

%\subsubsection{Spatial graphs}

%\subsubsection{Projections of spatial graphs}