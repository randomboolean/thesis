% \documentclass{article}
% \usepackage[utf8]{inputenc}
% \usepackage[english]{babel}
% \usepackage[T1]{fontenc}

% \usepackage{amsfonts}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{tikz}
% \usepackage{graphicx}
% \usepackage{color}
% \usepackage{enumitem}
% \usetikzlibrary{arrows}
% \usepackage{framed}
% \usepackage{arydshln}
% \usepackage{multirow}

% % no indent
% \setlength\parindent{0pt}

% % commands
% \usepackage{amsthm}
% \theoremstyle{definition}
% \newtheorem{definition}{Definition}[section]

% \newcommand{\cd}{\mathcal{D}}
% \newcommand{\ci}{\mathcal{I}}
% \newcommand{\bbr}{\mathbb{R}}

% \begin{document}


\textcolor{red}{TODO: Rework 1.1}

%\section{Disambiguations and definitions}

% This thesis manuscript is about deep learning on \emph{irregular domains}. So what does it mean exactly ?

%% start of <see below> comment

% The term \emph{deep learning}, as introduced in the previous chapter, refers to a family of learnable models based on deep neural networks. The inputs of these models are \emph{signals} of a specific type. Learning is made over a training dataset of such signals. Hence, the term \emph{domain} as in \emph{irregular domains} refers to the definition domain of these input signals.

%% Shoud be put later, must make disambiguation with "unstructured" as well

% In this section we recall the basic naming convention in~\ref{basic}, of some definitions in~\ref{regularity}, and categorize the models we will review by the tasks for what they are designed in~\ref{tasks}.

\subsection{Naming conventions}
\label{basic}

\subsubsection{Basic notions}

Let's recall the naming conventions of basic notions.

A \emph{function} $f: E \rightarrow F$ maps objects $x \in E$ to objects $y \in F$, as $y = f(x)$.\\
Its \emph{definition domain} $\cd_f = E$ is the set of objects onto which it is defined. We will often just use the term \emph{domain}.\\
%Objects of its domain $\cd_f$ are mapped to objects of its \emph{codomain} $\cd_f^c= F$.\\
We also say that $f$ is \emph{taking values} in its \emph{codomain} $F$.\\
The \emph{image per $f$} of the subset $U \subset E$, denoted $f(U)$, is $\{y \in F, \exists x \in E, y = f(x)\}$.\\
The \emph{image of $f$} is the image of its domain. We denote $\ci_f$.\\
% The \emph{fiber} of the object $y \in \ci_f$ is the object $x \in E$ such that $y = f(x)$.\\
% The \emph{inverse image per $f$} of the subset $V \subset F$, denoted $f^{-1}(V)$ is $\{x \in E, \exists y \in F, y = f(x)\}$.
A vector space $E$, which we will always assume to be finite-dimensional in our context, is defined as $\bbr^n$, and is equipped with pointwise addition and scalar multiplication.% TODO reword?

A \emph{signal} $s$ is a function taking values in a vector space. In other words, a signal can also be seen as a \emph{vector} with an \emph{underlying structure}, where the vector is composed from its image, and the underlying structure is defined by its \emph{domain}.\\

For example, images are signals define on a set of pixels. Typically, an image~$s$ in RGB representation is a mapping from pixels~$p$ to a 3d vector space, as $s_p = (r,g,b)$.

\textcolor{red}{TODO?: figure}
% quadillage , arrow ->, quadrillage remplie en 3 images
\begin{figure}

\end{figure}

\subsubsection{Graphs and graph signals}

%
\textcolor{red}{TODO: more defs on grid graphs and other graphs}
% need to define covariance graph, nearest neighbour
% Need to define grid graphs, regular grids from geometry, etc ...
% A regular grid graph is a nearest neighbor graph of a regular geometric grid
%

A \emph{graph} $G = (V, E)$ is defined as a set of nodes $V$, and a set of edges $E \subseteq\binom{V}{2}$. The words \emph{node} and \emph{vertex} will be used equivalently, but we will rather use the first.

A \emph{graph signal}, or \emph{graph-structured signal} is a signal defined on the nodes of a graph, for which the underlying structure is the graph itself.
A \emph{node signal} is a signal defined on a node, in which case it is a \emph{node embedding} in a vector space.

Although this is rarely seen, a signal can also be defined on the edges of a graph, or on an edge. We then coin it respectively \emph{dual graph signal}, or \emph{edge signal} / \emph{edge embedding}.

\emph{Graph-structured data} can refer to any of these type of signals.

\subsubsection{Data and datasets}

% Adjacency matrix, laplacian, etc ...
A dataset of signals is said to be \emph{static} if all its signals share the same underlying structure, it is said to be \emph{non-static} otherwise.\\
For image datasets, being non-static would mean that the dataset contains images of different sizes or different scales. For graph signal datasets, it would mean thats the underlying graph structures of the signals are different.

The point in specifying that objects of a dataset of a machine learning task are signals is that we can hope to leverage their underlying structure.

\textcolor{red}{TODO: figure}

\subsection{Disambiguation of the subject}

This thesis is entitled \emph{Deep learning models for data without a regular structure}.
So either the data of interest in this manuscript do not have any structure, or either their structure is not regular.

\subsubsection{Irregularly structured data}

By structured data, we mean that there exists an underlying structure over which the data is defined. This kind of data are usually modelized as signals defined over a domain. These domains are then composed of objects that are related together by some sort of structural properties. For example, pixels of images can be seen as located on a grid with integer spatial coordinates (a 2d cartesian grid graph).

It then come in handy to define the notions of structure and regularity with the help of graph signals.

\begin{definition}{Structure}\\
  Let $s: D \rightarrow F$ be a signal defined over a finite domain.\\
  An \emph{underlying structure} of the signal $s$ is a graph $G$ that has the domain of $s$ for nodes.\\
  A dataset is said to be \emph{structured}, if its objects can be modelized as signals with an underlying structure.\\
  It is said to be \emph{static} if all its objects share the same underlying structure, and \emph{non-static} otherwise.
\end{definition}

In other words, we chose to define ``structured data'' as ``graph-structured data'' by some graph. Hence we need to specify for which graphs this structure would be said to be regular, and for which it would not.

\begin{definition}{Regularity}\\
An underlying structure is said to be \emph{regular}, if it is a regular grid graph.
It is said to be \emph{irregular} otherwise.\\
A dataset is said to be \emph{regularly structured}, if the underlying structures of its objects are regular.
It is said to be \emph{irregularly structured} otherwise.
\end{definition}


\textcolor{red}{TODO: examples}
%% example images , example time series, example graph signals, example manifolds

\subsubsection{Unstructured data}

Data can also be unstructured. If the data is not yet embedded into a finite dimensional vector space, then we will be interested in embedding techniques used in representation learning. In the other case, it is often possible to fall back to the case of irregularly structured data. For example, vectors can be seen as signals defined over the canonical basis of the vector space, and the vectors of this basis can be related together by their covariances through the dataset. It is typical to use the graph structure that has the canonical basis for nodes, with edges obtained by covariance thresholding.

\textcolor{red}{TODO: examples}
%% give examples of unstructured data, graphs, scramble image datasets, etc..

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% END %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%
% DRAFTS
%
\textcolor{red}{What follows is a draft}


%\subsection{Theoretical results on regularity and convolutions}

% idea : regularity of a domain implies with poset
% prop: if a domain has a poset, define translations and convolutions
% proving the converse too

\subsection{Datasets}

\subsection{Tasks} %% probably too early
\label{tasks}



\subsection{Goals}

\subsection{Invariance}

In order to be observed, invariances must be defined relatively to an observation. Let's give a formal definition to support our discussion.

...

\subsection{Methods}
\label{methods}

%\end{document}