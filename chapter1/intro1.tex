\section*{Chapter overview}

In this chapter, we present notions related to our domains of interest. One of them, \emph{deep learning}, is the field of research that focuses around a particular class of functions: \emph{neural networks}. Since we try to employ a rigourous approach, we first define properly their input domain and their codomain, which can be modelized as \emph{tensor spaces}. In particular, we give original definitions of tensors in \secref{sec:tensors} that are appropriate for the study of neural networks. We also explain how data is handled and manipulated. We give definitions of some binary operations that are important for our study: \emph{tensor contraction}, and \emph{convolution}. In \secref{sec:nn}, we define neural networks, discuss their biological interpretation, present how they learn, and relate some historical advances. Then we introduce common layers, especially \emph{convolutional} ones for which we demonstrate a first result that helps toward our study. In the last section, \secref{sec:dlgraph}, we present the field of \emph{deep learning on graphs}. We start with definitions about graphs and signals, and then we describe use cases. Finally, we give a review of state-of-the-art models in two separate subsections, one on spectral methods, and the other one on vertex-domain methods.