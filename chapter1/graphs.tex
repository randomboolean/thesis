\section{Deep learning on graphs}

\subsection{Graph and signals}

We present the vocabulary, notation and conventions we will employ for graphs and signals.

\begin{definition}\textbf{Graph}\\
A \emph{graph} $G$ is defined as a couple of vertex and edge sets $\langle V,E \rangle$ \st $E \subset V^2$.
\end{definition}

The terms \emph{vertex} and \emph{node} are used interchangeably. Additionaly, we consider that a graph is always \emph{simple} \ie no two edges share the same set of vertices.
Unless stated otherwise, a graph is undirected, \ie $(u,v)$ and $(v,u)$ refer to the same edge. When it's not the case, it is called a \emph{digraph}.
We define the relation $u \sim v \Leftrightarrow (u,v) \in E$. We precise the graph if needed over the symbol $\overset{G}\sim$.
A \emph{path} is a sequence $v_1 \sim \cdots \sim v_r$. A graph is said to be \emph{connected} if there exists a path from any vertex to any other vertex.
We define the \emph{neighborhood} of a vertex as $\cn_u = \{v \in V, u \sim v\}$. For digraphs, it is equal to the union of the \emph{in}- and \emph{out}-neighborhoods. We only consider graphs without isolated vertex (a vertex with an empty neighborhood).
We also only consider \emph{weighted} graphs. That is, a graph $\gve$ is associated with a weight mapping $w: V^2 \to \bbr+$ \st $w(u,v) = 0 \Leftrightarrow u \nsim v$.
Its \emph{adjacency matrix} $A \in \bbr^{V \times V}$ is defined \wrt to a vertex ordering $V = \{v_1, \ldots, v_n\}$ as $A[i,j] = w(v_i,v_j)$. \figref{fig:graph} illustrates an example of a graph and its adjacency matrix.

\begin{figure}[h!tp]
\centering
\begin{tikzpicture}
\draw (0,0) -- (4,0) -- (4,4) -- (0,4) -- (0,0);
\node at (2,2){placeholder};
\end{tikzpicture}
\caption{Example of a graph}
\label{fig:graph}
\end{figure}

The \emph{order} of $G$ is equal to its number of vertices, possibly infinite.
The \emph{degree} of a vertex $v$ is equal to the number of edges it is attached to.
For digraphs the degree is the sum of the \emph{in}- and \emph{out}-degrees.
The \emph{degree} of $G$ refers to its max degree.
$G$ is said to be \emph{degree-regular} if all its vertices have the same degree.
Its \emph{degree matrix} $D$ (\wrt to a vertex ordering $V = \{v_1, \ldots, v_n\}$) is the diagonal matrix for which the diagonal entry corresponding to a vertex is the sum of the weights of the edges it is part of.
Its \emph{laplacian matrix} $L$ is the substraction $L = D-A$, which can be \emph{normalized} $L = I - D^{-\frac{1}2}AD^{-\frac{1}2}$, \emph{left-normalized} $L = I - D^{-1}A$, or \emph{right-normalized} $L = I - AD^{-1}$.
A subgraph of $G$ induced by a subset $U \subset V$ is the graph with vertex and edge set restricted by $U$.
A \emph{complete} graph is such that there exists an edge between any two vertices.

\begin{definition}\textbf{Grid graph}\\
A \emph{grid graph} $\gve$ is 
such that $V \cong \bbz^2$, $v_1 \sim v_2 \Rightarrow \|v_2 -v_1\|_\infty \in \{0, 1\}$ and either one of the following is true:
\begin{gather*}
\left\{
  \begin{array}{l}
    (i_1,j_1) \sim (i_2,j_2) \Leftrightarrow |i_2 - i_1| \XOR |j_2 - j_1| \quad \text{($4$ neighbours)}\\
    (i_1,j_1) \sim (i_2,j_2) \Leftrightarrow |i_2 - i_1| \AND |j_2 - j_1| \quad \text{($4$ neighbours)}\\
    (i_1,j_1) \sim (i_2,j_2) \Leftrightarrow |i_2 - i_1| \OR |j_2 - j_1| \quad \text{($8$ neighbours)}
  \end{array}
\right.
\end{gather*}

A \emph{(rectangular) grid graph} of size $n \times m$ is the subgraph of a grid graph induced by $\llbracket 1, n \rrbracket \times \llbracket 1, m \rrbracket$. A \emph{square grid graph} is a rectangular grid graph of square size.
\end{definition}

\begin{definition}\textbf{Bipartite graph}\\
A graph is said to be \emph{bipartite} if its vertex set is a disjoint union of two sets $V = V_1 \cup V_2$ \st $$u \sim v \Rightarrow (u,v) \in V_1 \times V_2 \vee (u,v) \in V_2 \times V_1$$
\end{definition}

Its \emph{bipartite-adjacency} matrix $A \in \bbr^{V_1 \times V_2}$ is a rectangular matrix defined \wrt to a vertex ordering $V_1 = \{u_1, \ldots, u_n\}$, $V_2 = \{v_1, \ldots, v_n\}$ and weight mapping $w$ as $A[i,j] = w(u_i,v_j)$.

\begin{definition}\textbf{Signal}\\
A \emph{signal} on $V$, $s \in \cs(V)$, is a function $s: V \rightarrow \bbr$.
The \emph{signal space} $\cs(V)$ is the linear space of signals on $V$.
\end{definition}

\begin{remark}
In particular, a vector space, and more generally a tensor space, are finite-dimensional signal spaces on any of their bases.
\end{remark}

A \emph{graph signal} on a graph $\gve$ is a signal on its vertex set $V$. We denote by $\cs(G)$ or $\cs(V)$ the graph signal space. $G$ can be referred as the \emph{underlying structure} of $\cs(V)$.
An \emph{entry} of a signal $s$ is an image by $s$ of some $v \in V$ and we denote $s[v]$. If~$v$~is represented by a $n$-tuple, we can also write $s[v_1, v_2, \ldots, v_n]$.
The \emph{support} of a signal $s \in \cs(V)$ is the subset $\supp(s) \subset V$ on which $s \neq 0$.
For space of signals that aren't real-valued, their codomain~$\bbe$ is precised in the subscript~$\cs_{\bbe}(V)$.

\subsection{Learning tasks}

There are many tasks related to deep learning on graphs.%:
% \begin{itemize}[nolistsep,noitemsep]
%   \item supervised classification of graph signals
%   \item supervised classification of graphs
%   \item semi-supervised classification of node signals
%   \item semi-supervised representation learning of nodes
% \end{itemize}

\paragraph{Supervised classification of graph signals}
This is the classical application of deep learning transposed to graph signals, rather than image or audio signals. It is the principled targeted task we will have in mind in the course of the remainder of this manuscript. Given a graph $\gve$ and an input signal $x \in \cs(G)$ the goal is to classify~$x$. If there are~$c$ possible classes, a neural network~$f$ outputs a vector $y = f(x)$ of dimension~$c$, and its dimension with the biggest weight determines the predicted class. Indeed, a standard MLP can be trained on a dataset of graph signals. However, an MLP wouldn't take the graph structure $G$ into consideration. By similarity with CNNs that leverage the grid structure of images to achieve better performances than MLPs, a challenge is to define a neural network on graph signals that can leverage~$G$. We review some models from the litterature in \secref{sec:spec} and in \secref{sec:vert}. We develop an algebraic understanding in \chapref{chap:2} of why and how they should work, and also propose our own models and point of view in \chapref{chap:3}.

\paragraph{Semi-supervised classification of nodes}
This task is in some way obtained from a transposed perspective of the previous one. Given a dataset of graph signals, represented as a matrix $X \in \bbr^{n \times N}$, where the rows represent the nodes, and the columns represent the signals, the goal is to classify the nodes. This amounts to classify the rows, whereas the previous task amounts to classify the columns. As opposed to the previous one, this task is \emph{transductive} \ie node data from the test set are available during training (but their labels are not), and it is \emph{semi-supervised} \ie some node labels of the train set are unknown. This allows to learn on much more data than if we were restricted to labeled data. In this task, the edges connect learning samples, however in the previous one, the edges were connecting features of learning samples. This is this edge relationship between learning samples that renders the semi-supervised approach possible. This task have received much more attention than the previous one in the litterature. This may be because the datasets

\paragraph{Other learning tasks}
In this manuscipt, we are less interested in other deep learning tasks related to graphs, so we briefly discuss them here. One is supervised classification of graphs, which is different than classifying graph signals. Examples include \citep{niepert2016learning,tixier2017classifying}. Another interesting task is the semi-supervised representation learning of nodes, which tackles the challenge to learn a linear representation of nodes. A common approach, derived from Word2Vec \citep{mikolov2013efficient,mikolov2013distributed}, is called Node2Vec \citep{grover2016node2vec}, and was later improved in graphSAGE \citep{hamilton2017inductive}. A review on this subject is done by \cite{hamilton2017representation}.

\subsection{Spectral methods}
\label{sec:spec}

Spectral methods are based on spectral graph theory \citep{chung1996spectral}, which is interested in the properties of graphs explained through the eigenvalues of their laplacian matrix. Since the laplacian $L$ of a graph $\gve$ is real symmetric, it can be diagonalized in an orthonormal eigenvector basis as $\Lambda = ULU^T$, where $\Lambda = \diag(\lambda_1, \ldots, \lambda_n)$, and $\lambda_1 \le \cdots \le \lambda_n$.
In the cases corresponding to graph structures of euclidean domains \ie when $L$ is the laplacian of a circulant grid graph, one can verify that $U$ can be the DFT matrix which defines the Discrete Fourier Transform. By extension to general graphs, for every signals $s \in \cs(G)$, the \emph{Graph Fourier Transform} (GFT) is defined as $\hat{s} = Us$ \citep{shuman2013emerging}.

The GFT shares many similar properties with the DFT. For example, let a signal $s \in \cs(V)$, the laplacian $L$ and the positive weight mapping $w$ of the edges we have 
\begin{align}
Ls[u] &= \displaystyle\sum_{v \in V} w(u,v)(s[u] - s[v])\\
s^TLs &= \displaystyle\sum_{u \in V}\sum_{v \in V} w(u,v)s[u](s[u] - s[v])\nonumber\\
&= \displaystyle \frac{1}2\sum_{u \in V}\sum_{v \in V} w(u,v)s[u](s[u] - s[v]) + \frac{1}2\sum_{v \in V}\sum_{u \in V} w(v,u)s[v](s[v] - s[u])\nonumber\\
&=  \displaystyle\sum_{u \in V}\sum_{v \in V} \frac{w(u,v)}2(s[u] - s[v])^2
\end{align}
That is, $s^TLs$ is some sort of measure of \emph{smoothness} of the signal $s$, penalized by the weights on the edges of the graph $G$. The bigger is $w(u,v)$, the closest $s(u)$ and $s(v)$ must be to lower this smoothness measure. %Since $L$ is symmetric, its eigenvalues are non-negative real numbers, and $L$ can be diagonalized in an orthonormal eigenvector basis as $\Lambda = U^TLU$. Denote $\hat{s} = U^Ts$, and $(\lambda_i)_i$ the eigenvalues, the smoothness measure rewrites
In the spectral domain, this smoothness rewrites:
\begin{align}
s^TLs = \hat{s}^T\Lambda\hat{s} = \displaystyle\sum_{i=1}^n \lambda_i \hat{s}[i]^2
\end{align}
In other words, the smoothness of $s$ can be read off the coordinates of $\hat{s}$, and decay in the spectral domain relate to smoothness in the vertex domain.



\subsection{Vertex-domain methods}
\label{sec:vert}


% \subsection{Graphs in deep learning}

% \todo{below}

% We come across the notion of graphs several times in deep learning:
% \begin{itemize}
% \item Connections between two layers of a deep learning model can be represented as a bipartite graph, the \emph{connectivity graph}. It encodes how the information is propagated through a layer to another. See \secref{con_graph}.
% \item Neural architectures can be represented by a graph. In particular, a computation graph is used by deep learning programming languages to keep track of the dependencies between layers of a deep learning model, in order to compute forward and back-propagation. See \secref{comp_graph}.
% \item A graph can represent the underlying structure of an object (often a vector or a signal). The nodes represent its features, and the edges represent some structural property. See \secref{inductive_graph}.
% \item Datasets can also be graph-structured. The nodes represent the objects of the dataset, and its edge represent some sort of relation between them. See \secref{transductive_graph}.
% \end{itemize}

% %\subsection{Graphs related to models}

% \subsubsection{Connectivity graph}
% \label{con_graph}

% A Connectivity graph is the bipartite graph whose adjacency matrix is the connectivity matrix of a layer of neurons.
% %$U = \{u_1, u_2, \ldots, u_n\}$
% Formally, given a linear part of a layer, let $\textbf{x}$ and $\textbf{y}$ be the input and output signals, $n$ the size of the set of input neurons $N = \{u_1, u_2, \ldots, u_n\}$, and $m$ the size of the set of output neurons $M = \{v_1, v_2, \ldots, v_m\}$. This layer implements the equation $y = \Theta x$ where $\Theta$ is a $n \times m$ matrix.

% \begin{definition}
% {The \emph{connectivity graph} $G = (V,E)$ is defined such that $V = N \cup M$ and $E = \{(u_i,v_j) \in  N \times M, \Theta_{ij} \neq 0 \} $.}
% \end{definition}

% I.e. the connectivity graph is obtained by drawing an edge between neurons for which $\Theta_{ij} \neq 0$.
% For instance, in the special case of a complete bipartite graph, we would obtain a dense layer. 
% Connectivity graphs are especially useful to represent partially connected layers, for which most of the $\Theta_{ij}$ are $0$. 
% For example, in the case of layers characterized by a small local receptive field, the connectivity graph would be sparse, and output neurons would be connected to a set of input neurons that corresponds to features that are close together in the input space. \figref{con_ex} depicts some examples.

% \begin{figure}[h]
%   \begin{center}
%     \begin{tikzpicture}
%       \tikzstyle{every node} = [draw, circle, thick, inner sep = 2pt]
%       \foreach \y in {0,...,4}{
%         \pgfmathtruncatemacro{\yplusone}{5 - \y}
%         \node(a\y) at (0,.6*\y) {\footnotesize\yplusone};
%       }
%       \foreach \y in {0,...,4}{
%         \pgfmathtruncatemacro{\yplusone}{5 - \y}
%         \node(\y) at (2,.6*\y) {\footnotesize\yplusone};
%       }

%       \foreach \x in {0,...,4}{
%         \foreach \y in {0,...,4}{
%           \path[opacity=0.5] (a\x) edge (\y);
%         }
%       }
%     \end{tikzpicture}
%   \end{center}
%   \caption{Examples}
%   \label{con_ex}
% \end{figure}

% \todo{\figref{con_ex}. It's just a placeholder right now}


% Connectivity graphs also allow to graphically modelize how weights are tied in a neural layer. Let's suppose the $\Theta_ij$ are taking their values only into the finite set $K = \{w_1, w_2, \ldots, w_\kappa\}$ of size $\kappa$, which we will refer to as the \emph{kernel} of \emph{weights}. Then we can define a labelling of the edges $s: E \rightarrow K$. $s$ is called the \emph{weight sharing scheme} of the layer. This layer can then be formulated as $\displaystyle \forall v \in M, y_v = \sum_{u \in N, (u,v) \in E} w_{s(u,v)} x_u$. \figref{cnn} depicts the connectivity graph of a 1-d convolution layer and its weight sharing scheme.

% \begin{figure}[h]
%   \begin{center}
%     \begin{tikzpicture}
%       \tikzstyle{every node} = [draw, circle, thick, inner sep = 2pt]
%       \foreach \y in {0,...,4}{
%         \pgfmathtruncatemacro{\yplusone}{5 - \y}
%         \node(a\y) at (0,.6*\y) {\footnotesize\yplusone};
%       }
%       \foreach \y in {0,...,4}{
%         \pgfmathtruncatemacro{\yplusone}{5 - \y}
%         \node(\y) at (2,.6*\y) {\footnotesize\yplusone};
%       }
%       \path[opacity=0.5]
%       (a0) edge (0);
%       \path[dashed]
%       (a0) edge (1);
%       \path[dotted]
%       (a1) edge (0);
%       \path[opacity=0.5]
%       (a1) edge (1);
%       \path[dashed]
%       (a1) edge (2);
%       \path[dotted]
%       (a2) edge (1);
%       \path[opacity=0.5]
%       (a2) edge (2);
%       \path[dashed]
%       (a2) edge (3);
%       \path[dotted]
%       (a3) edge (2);
%       \path[opacity=0.5]
%       (a3) edge (3);
%       \path[dashed]
%       (a3) edge (4);
%       \path[dotted]
%       (a4) edge (3);
%       \path[opacity=0.5]
%       (a4) edge (4);
%     \end{tikzpicture}
%   \end{center}
%   \caption{Depiction of a 1D-convolutional layer and its weight sharing scheme.}
%   \label{cnn}
% \end{figure}


% \todo{Add weight sharing scheme in \figref{cnn}}

% \subsubsection{Computation graph}
% \label{comp_graph}

% %\subsection{Graphs related to data}

% \subsubsection{Underlying graph structure and signals}
% \label{inductive_graph}

% \subsubsection{Graph-structured dataset}
% \label{transductive_graph}

% %transductive vs inductive