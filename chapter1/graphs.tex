\section{Deep learning on graphs}

\subsection{Graph and signals}

We present the vocabulary, notation and conventions we will employ for graphs and signals.

\begin{definition}\textbf{Graph}\\
A \emph{graph} $G$ is defined as a couple of vertex and edge sets $\langle V,E \rangle$ \st $E \subset V^2$.
\end{definition}

The terms \emph{vertex} and \emph{node} are used interchangeably. Additionaly, we consider that a graph is always \emph{simple} \ie no two edges share the same set of vertices.
Unless stated otherwise, a graph is undirected, \ie $(u,v)$ and $(v,u)$ refer to the same edge. When it's not the case, it is called a \emph{digraph}.
We define the relation $u \sim v \Leftrightarrow (u,v) \in E$. We precise the graph if needed over the symbol $\overset{G}\sim$.
A \emph{path} is a sequence $v_1 \sim \cdots \sim v_r$. A graph is said to be \emph{connected} if there exists a path from any vertex to any other vertex.
We define the \emph{neighborhood} of a vertex as $\cn_u = \{v \in V, u \sim v\}$. For digraphs, it is equal to the union of the \emph{in}- and \emph{out}-neighborhoods. We only consider graphs without isolated vertex (a vertex with an empty neighborhood).
We also only consider \emph{weighted} graphs. That is, a graph $\gve$ is associated with a weight mapping $w: V^2 \to \bbr+$ \st $w(u,v) = 0 \Leftrightarrow u \nsim v$.
If $G$ is finite. Its \emph{adjacency matrix} $A \in \bbr^{V \times V}$ is defined \wrt to a vertex ordering $V = \{v_1, \ldots, v_n\}$ as $A[i,j] = w(v_i,v_j)$. \figref{fig:graph} illustrates an example of a graph and its adjacency matrix.

\begin{figure}[h!tp]
\centering
\begin{tikzpicture}
\draw (0,0) -- (4,0) -- (4,4) -- (0,4) -- (0,0);
\node at (2,2){placeholder};
\end{tikzpicture}
\caption{Example of a graph}
\label{fig:graph}
\end{figure}

The \emph{order} of $G$ is equal to its number of vertices, possibly infinite.
The \emph{degree} of a vertex $v$ is equal to the number of edges it is attached to.
For digraphs the degree is the sum of the \emph{in}- and \emph{out}-degrees.
The \emph{degree} of $G$ refers to its max degree.
$G$ is said to be \emph{degree-regular} if all its vertices have the same degree.
If it is finite, its \emph{degree matrix} $D$ (\wrt to a vertex ordering $V = \{v_1, \ldots, v_n\}$) is the diagonal matrix for which the diagonal entry corresponding to a vertex is the sum of the weights of the edges it is part of.
Its \emph{laplacian matrix} $L$ is the substraction $L = D-A$, which can be \emph{normalized} $L = I - D^{-\frac{1}2}AD^{-\frac{1}2}$, \emph{left-normalized} $L = I - D^{-1}A$, or \emph{right-normalized} $L = I - AD^{-1}$.
A subgraph of $G$ induced by a subset $U \subset V$ is the graph with vertex and edge set restricted by $U$. The \emph{complement} graph $G^C$ shares the same vertex set but $u \overset{G^C}\sim v \Leftrightarrow u \overset{G}\sim v$.
A \emph{complete} graph is such that there exists an edge between any two vertices.

% \begin{definition}\textbf{Grid graph}\\
% A \emph{grid graph} $\gve$ is 
% such that $V \cong \bbz^2$, $v_1 \sim v_2 \Rightarrow \|v_2 -v_1\|_\infty \in \{0, 1\}$ and either one of the following is true:
% \begin{gather*}
% \left\{
%   \begin{array}{l}
%     (i_1,j_1) \sim (i_2,j_2) \Leftrightarrow |i_2 - i_1| \XOR |j_2 - j_1| \quad \text{($4$ neighbours)}\\
%     (i_1,j_1) \sim (i_2,j_2) \Leftrightarrow |i_2 - i_1| \AND |j_2 - j_1| \quad \text{($4$ neighbours)}\\
%     (i_1,j_1) \sim (i_2,j_2) \Leftrightarrow |i_2 - i_1| \OR |j_2 - j_1| \quad \text{($8$ neighbours)}
%   \end{array}
% \right.
% \end{gather*}

% A \emph{(rectangular) grid graph} of size $n \times m$ is the subgraph of a grid graph induced by $\llbracket 1, n \rrbracket \times \llbracket 1, m \rrbracket$. A \emph{square grid graph} is a rectangular grid graph of square size.
% \end{definition}

\begin{definition}\textbf{Grid graph}\\
Let a graph $\gve$ such that the expression $u \sim v \Leftrightarrow \|u-v\|_1 = 1$ makes sense. $G$ can be called:
\begin{itemize}[nolistsep,noitemsep]
\item a \emph{grid graph} if $V = \bbz^2$
\item a \emph{finite grid graph} if $\exists (n,m) \in \bbz^2, V = \llbracket 1, n \rrbracket \times \llbracket 1, m \rrbracket$
\item a \emph{circulant grid graph} if $\exists (n,m) \in \bbz^2, V = \bbz /n \bbz \times \bbz /m \bbz$
\end{itemize}
\end{definition}

\begin{definition}\textbf{Bipartite graph}\\
A graph is called \emph{bipartite} if its vertex set is a disjoint union $V = V_1 \cup V_2$ \st $$u \sim v \Rightarrow (u,v) \in V_1 \times V_2 \vee (u,v) \in V_2 \times V_1$$
\end{definition}

If it is finite, its \emph{bipartite-adjacency} matrix $A \in \bbr^{V_1 \times V_2}$ is a rectangular matrix defined \wrt to a vertex ordering $V_1 = \{u_1, \ldots, u_n\}$, $V_2 = \{v_1, \ldots, v_n\}$ and weight mapping $w$ as $A[i,j] = w(u_i,v_j)$.

\begin{definition}\textbf{Signal}\\
A \emph{signal} on $V$, $s \in \cs(V)$, is a function $s: V \rightarrow \bbr$.
The \emph{signal space} $\cs(V)$ is the linear space of signals on $V$.
\end{definition}

\begin{remark}
In particular, a vector space, and more generally a tensor space, are finite-dimensional signal spaces on any of their bases.
\end{remark}

A \emph{graph signal} on a graph $\gve$ is a signal on its vertex set $V$. We denote by $\cs(G)$ or $\cs(V)$ the graph signal space. $G$ can be referred as the \emph{underlying structure} of $\cs(V)$.
An \emph{entry} of a signal $s$ is an image by $s$ of some $v \in V$ and we denote $s[v]$. If~$v$~is represented by a $n$-tuple, we can also write $s[v_1, v_2, \ldots, v_n]$.
The \emph{support} of a signal $s \in \cs(V)$ is the subset $\supp(s) \subset V$ on which $s \neq 0$.
For space of signals that aren't real-valued, their codomain~$\bbe$ is precised in the subscript~$\cs_{\bbe}(V)$.

\subsection{Learning tasks}

There are many tasks related to deep learning on graphs.%:
% \begin{itemize}[nolistsep,noitemsep]
%   \item supervised classification of graph signals
%   \item supervised classification of graphs
%   \item semi-supervised classification of node signals
%   \item semi-supervised representation learning of nodes
% \end{itemize}

\paragraph{Supervised classification of graph signals}
This is the classical application of deep learning transposed to graph signals, rather than image or audio signals. It is the principled targeted task we will have in mind in the course of the remainder of this manuscript. Given a graph $\gve$ and an input signal $x \in \cs(G)$ the goal is to classify~$x$. If there are~$c$ possible classes, a neural network~$f$ outputs a vector $y = f(x)$ of dimension~$c$, and its dimension with the biggest weight determines the predicted class. Indeed, a standard MLP can be trained on a dataset of graph signals. However, an MLP wouldn't take the graph structure $G$ into consideration. By similarity with CNNs that leverage the grid structure of images to achieve better performances than MLPs, a challenge is to define a neural network on graph signals that can leverage~$G$. We review some models from the litterature in \secref{sec:spec} and in \secref{sec:vert}. We develop an algebraic understanding in \chapref{chap:2} of why and how they should work, and also propose our own models and point of view in \chapref{chap:3}.

\paragraph{Semi-supervised classification of nodes}
This task is in some way obtained from a transposed perspective of the previous one. Given a dataset of graph signals, represented as a matrix $X \in \bbr^{n \times N}$, where the rows represent the nodes, and the columns represent the signals, the goal is to classify the nodes. This amounts to classify the rows, whereas the previous task amounts to classify the columns. As opposed to the previous one, this task is \emph{transductive} \ie node data from the test set are available during training (but their labels are not), and it is \emph{semi-supervised} \ie some node labels of the train set are unknown. This allows to learn on much more data than if we were restricted to labeled data. In this task, the edges connect learning samples, however in the previous one, the edges were connecting features of learning samples. This is this edge relationship between learning samples that renders the semi-supervised approach possible. This task have received much more attention than the previous one in the litterature. This may be because the datasets

\paragraph{Other learning tasks}
In this manuscipt, we are less interested in other deep learning tasks related to graphs, so we briefly discuss them here. One is supervised classification of graphs, which is different than classifying graph signals. Examples include \citep{niepert2016learning,tixier2017classifying}. Another interesting task is the semi-supervised representation learning of nodes, which tackles the challenge to learn a linear representation of nodes. A common approach, derived from Word2Vec \citep{mikolov2013efficient,mikolov2013distributed}, is called Node2Vec \citep{grover2016node2vec}, and was later improved in graphSAGE \citep{hamilton2017inductive}. A review on this subject is done by \cite{hamilton2017representation}.

\subsection{Spectral methods}
\label{sec:spec}

Spectral methods are based on spectral graph theory \citep{chung1996spectral} which aims at characterizing structral properties of a graph $\gve$ through the eigenvalues of the laplacian matrix $L$. In particular, since it is hermitian, it admits a complete set of normalized eigenvectors. By fixing a normalized eigenvector basis ordered in the rows of $U$ (by ascending eigenvalues), $U$ is used to define the \emph{Graph Fourier Transform} (GFT) of a signal $s \in \cs(G)$ \citep{shuman2013emerging}, and the conjugate-transpose $U^*$ defines the inverse GFT. We write
\begin{align}
\widehat{s} &= Us\\
\widetilde{s} &= U^*s
\end{align}

\begin{remark}
The GFT extends the notion of \emph{Discrete Fourier Transform} (DFT) to general graphs, since that for circulant grid graphs $U$ can be the DFT matrix.
\end{remark}

By analogy with the convolution theorem, a convolution can be defined as pointwise multiplication, denoted $\cdot$, in the spectral domain of the graph \citep{hammond2011wavelets}. For $s, g \in \cs(G)$, we have:
\begin{gather}
s \ast g = \widetilde{\widehat{s} \cdot \widehat{g}} \label{eq:sc}
\end{gather}

This expression can be used to define convolutional layers and spectral CNNs on graphs. However, \cite{bruna2013spectral} pointed out that \eqref{eq:sc} would generate filters with $\co(n)$ weights, where $n$ is the order of $G$. So they proposed to learn filters $\theta$ with only $\co(1)$ weights and then to smoothly interpolate the remaining weights as $g = K \theta$, where $K$ is a linear smoother. They motivate their construction by the fact that smooth multipliers in the spectral domain should simulate local operations in the vertex domain. To elaborate a bit on this, note that we have:
\begin{align}
Ls[u] &= \displaystyle\sum_{v \in V} w(u,v)(s[u] - s[v])
\end{align}
And so,
\begin{align}
s^TLs &= \displaystyle\sum_{u \in V}\sum_{v \in V} w(u,v)s[u](s[u] - s[v])\nonumber\\
&= \displaystyle \frac{1}2\sum_{u \in V}\sum_{v \in V} w(u,v)s[u](s[u] - s[v]) + \frac{1}2\sum_{v \in V}\sum_{u \in V} w(v,u)s[v](s[v] - s[u])\nonumber\\
&=  \displaystyle\sum_{u \in V}\sum_{v \in V} \frac{w(u,v)}2(s[u] - s[v])^2 \label{eq:smooth}
\end{align}
That is, $s^TLs$ is some sort of measure of \emph{smoothness} of the signal $s$, penalized by the weights $w$. The bigger is $w(u,v)$, the closest $s(u)$ and $s(v)$ must be to lower the smoothness \eqref{eq:smooth}. Since $L$ is symmetric, its eigenvalues are non-negative real numbers, and $U$ diagonalizes $L$ as $\Lambda = ULU^*$. Denote $(\lambda_i)_i$ the eigenvalues, the smoothness measure rewrites:
\begin{align}
s^TLs = \widehat{s}^*\Lambda\widehat{s} = \displaystyle\sum_{i=1}^n \lambda_i \widehat{s}[i]^2
\end{align}
Therefore, as they pointed out, smoothness of $s$ can be read off the coordinates of $\hat{s}$, like for the DFT. Moreover, spectral multipliers modulate its smoothness, and decay in the spectral domain is related to smoothness in the vertex domain. But contrary to their conjecture, smoothness in the spectral domain is not necessary related to decay is the vertex domain (and so to some form of locality). For instance, since the laplacian $L^C$ of the complement graph $G^C$ commutes with $L$, it can share the same eigenvector basis $U$, and thus define the same GFT, but their notion of locality in the vertex domain are opposed. Another drawback is that this methods requires computing a laplacian eigenbasis, which complexity is at least $\co(n^2)$ as there is no equivalent of the Fast Fourier Transform (FFT).

Then, \cite{defferrard2016convolutional} remedy to these issues by proposing an approximation of the GFT based on the Chebychev polynomials, denoted by $(T_i)_i$, where $i$ is the polynomial order.
That is, their proposed filters are in the form
\begin{gather}
g_\theta(L) = \sum_{i=0}^k \theta[i] \h{2} T_i(L)
\end{gather}
which are spectral multipliers because, thanks to $T_i(L) = T_i(U^*\Lambda U) = U^*T_i(\Lambda)U$, we have:
\begin{align}
g_\theta(L)s &= g_\theta(U^*\Lambda U)s
= U^* g_\theta(\Lambda) Us\nonumber\\
&= \widetilde{g_\theta(\Lambda) \textbf1} \ast s
\end{align}

These filters enjoy locality properties and their complexity remains $\co(1)$.

However, it is hard to evaluate if a model performs well on the task of supervised classification of graph signal, because there are not much known datasets in the litterature for which the given graph domain hold enough information.

For example, \citeauthor{defferrard2016convolutional} built a graph signal dataset from a text categorization dataset called 20NEWS \citep{joachims1996probabilistic}. Each text is represented as a word2vec vector, and features are linked by edges with their nearest neighbors. However, their model (ChebNet32) fails to surpass Multinomial Naive Bayes (MNB). Moreover, even though they report that their model beat MLPs, our experiments show the contrary. In results we report in \tabref{tab:20}, we see that a lighter MLP, composed of a single Fully-Connected~(FC) layer with ReLU and 20\% dropout outperforms ChebNet32. We replicated their preprocessing phase from the code on their github repository and averaged our results on 10 runs of 20 epochs.

\begin{table}[H]
  \caption{Accuracies on 20NEWS}
  \begin{center}
    \bgroup
    \def\arraystretch{1.5}%  1 is the default, change whatever you need
    \begin{tabular}{|c|c|c|c|c|}
      \hline
      MNB & FC2500 & FC2500-FC500 & ChebNet32 & FC500\\
      \hline
      68.51\%$^a$ & 64.64\%$^a$ & 65.76\%$^a$ & 68.26\%$^a$ & \textbf{71.46$\pm$0.08\%}$^b$\\
      \hline
    \end{tabular}
    \egroup
  \end{center}
\begin{flushleft}
\footnotesize{
$^a$ As reported in \cite{defferrard2016convolutional}\\
$^b$ From our experiments.
}
\end{flushleft}
  \label{tab:20}
\end{table}

Despite the significant theoretical contribution, this result stress out the importance of the practical graph used to support the convolution, a point that they also discussed. \cite{henaff2015deep} propose supervised graph estimation techniques, but a better graph signal dataset would be one that come with an already suitable graph, that of current literature is still lacking.

On the other hand, attention in the domain has shifted toward the task of semi-supervised classification of nodes, where good datasets are not lacking. \cite{kipf2016semi} have transposed ChebNet to this task, and propose a simplification.

\todo{suite}
\todo{and retreive in chap3 part that was removed}

% They remark that spectral multipliers $\widehat{g_\theta}$, parameterized by $\theta$, can be rewritten in the form
% \begin{gather}
% g_\theta \ast s = U^*\diag{\widehat{g_\theta}}Us
% \end{gather}
% And if $g_\theta$ is a polynomial in $\Lambda = ULU^*$, we have


\subsection{Vertex-domain methods}
\label{sec:vert}


% \subsection{Graphs in deep learning}

% \todo{below}

% We come across the notion of graphs several times in deep learning:
% \begin{itemize}
% \item Connections between two layers of a deep learning model can be represented as a bipartite graph, the \emph{connectivity graph}. It encodes how the information is propagated through a layer to another. See \secref{con_graph}.
% \item Neural architectures can be represented by a graph. In particular, a computation graph is used by deep learning programming languages to keep track of the dependencies between layers of a deep learning model, in order to compute forward and back-propagation. See \secref{comp_graph}.
% \item A graph can represent the underlying structure of an object (often a vector or a signal). The nodes represent its features, and the edges represent some structural property. See \secref{inductive_graph}.
% \item Datasets can also be graph-structured. The nodes represent the objects of the dataset, and its edge represent some sort of relation between them. See \secref{transductive_graph}.
% \end{itemize}

% %\subsection{Graphs related to models}

% \subsubsection{Connectivity graph}
% \label{con_graph}

% A Connectivity graph is the bipartite graph whose adjacency matrix is the connectivity matrix of a layer of neurons.
% %$U = \{u_1, u_2, \ldots, u_n\}$
% Formally, given a linear part of a layer, let $\textbf{x}$ and $\textbf{y}$ be the input and output signals, $n$ the size of the set of input neurons $N = \{u_1, u_2, \ldots, u_n\}$, and $m$ the size of the set of output neurons $M = \{v_1, v_2, \ldots, v_m\}$. This layer implements the equation $y = \Theta x$ where $\Theta$ is a $n \times m$ matrix.

% \begin{definition}
% {The \emph{connectivity graph} $G = (V,E)$ is defined such that $V = N \cup M$ and $E = \{(u_i,v_j) \in  N \times M, \Theta_{ij} \neq 0 \} $.}
% \end{definition}

% I.e. the connectivity graph is obtained by drawing an edge between neurons for which $\Theta_{ij} \neq 0$.
% For instance, in the special case of a complete bipartite graph, we would obtain a dense layer. 
% Connectivity graphs are especially useful to represent partially connected layers, for which most of the $\Theta_{ij}$ are $0$. 
% For example, in the case of layers characterized by a small local receptive field, the connectivity graph would be sparse, and output neurons would be connected to a set of input neurons that corresponds to features that are close together in the input space. \figref{con_ex} depicts some examples.

% \begin{figure}[h]
%   \begin{center}
%     \begin{tikzpicture}
%       \tikzstyle{every node} = [draw, circle, thick, inner sep = 2pt]
%       \foreach \y in {0,...,4}{
%         \pgfmathtruncatemacro{\yplusone}{5 - \y}
%         \node(a\y) at (0,.6*\y) {\footnotesize\yplusone};
%       }
%       \foreach \y in {0,...,4}{
%         \pgfmathtruncatemacro{\yplusone}{5 - \y}
%         \node(\y) at (2,.6*\y) {\footnotesize\yplusone};
%       }

%       \foreach \x in {0,...,4}{
%         \foreach \y in {0,...,4}{
%           \path[opacity=0.5] (a\x) edge (\y);
%         }
%       }
%     \end{tikzpicture}
%   \end{center}
%   \caption{Examples}
%   \label{con_ex}
% \end{figure}

% \todo{\figref{con_ex}. It's just a placeholder right now}


% Connectivity graphs also allow to graphically modelize how weights are tied in a neural layer. Let's suppose the $\Theta_ij$ are taking their values only into the finite set $K = \{w_1, w_2, \ldots, w_\kappa\}$ of size $\kappa$, which we will refer to as the \emph{kernel} of \emph{weights}. Then we can define a labelling of the edges $s: E \rightarrow K$. $s$ is called the \emph{weight sharing scheme} of the layer. This layer can then be formulated as $\displaystyle \forall v \in M, y_v = \sum_{u \in N, (u,v) \in E} w_{s(u,v)} x_u$. \figref{cnn} depicts the connectivity graph of a 1-d convolution layer and its weight sharing scheme.

% \begin{figure}[h]
%   \begin{center}
%     \begin{tikzpicture}
%       \tikzstyle{every node} = [draw, circle, thick, inner sep = 2pt]
%       \foreach \y in {0,...,4}{
%         \pgfmathtruncatemacro{\yplusone}{5 - \y}
%         \node(a\y) at (0,.6*\y) {\footnotesize\yplusone};
%       }
%       \foreach \y in {0,...,4}{
%         \pgfmathtruncatemacro{\yplusone}{5 - \y}
%         \node(\y) at (2,.6*\y) {\footnotesize\yplusone};
%       }
%       \path[opacity=0.5]
%       (a0) edge (0);
%       \path[dashed]
%       (a0) edge (1);
%       \path[dotted]
%       (a1) edge (0);
%       \path[opacity=0.5]
%       (a1) edge (1);
%       \path[dashed]
%       (a1) edge (2);
%       \path[dotted]
%       (a2) edge (1);
%       \path[opacity=0.5]
%       (a2) edge (2);
%       \path[dashed]
%       (a2) edge (3);
%       \path[dotted]
%       (a3) edge (2);
%       \path[opacity=0.5]
%       (a3) edge (3);
%       \path[dashed]
%       (a3) edge (4);
%       \path[dotted]
%       (a4) edge (3);
%       \path[opacity=0.5]
%       (a4) edge (4);
%     \end{tikzpicture}
%   \end{center}
%   \caption{Depiction of a 1D-convolutional layer and its weight sharing scheme.}
%   \label{cnn}
% \end{figure}


% \todo{Add weight sharing scheme in \figref{cnn}}

% \subsubsection{Computation graph}
% \label{comp_graph}

% %\subsection{Graphs related to data}

% \subsubsection{Underlying graph structure and signals}
% \label{inductive_graph}

% \subsubsection{Graph-structured dataset}
% \label{transductive_graph}

% %transductive vs inductive